{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>Prise en main de 2 librairies de deep-learning : Keras et Pytorch</b>\n",
    "\n",
    "Pour cette prise en main, nous allons nous limiter au perceptron multicouche (MLP).  \n",
    "On va reprendre les exemples déjà utilisés :\n",
    "- Iris\n",
    "- MNIST\n",
    "- GTSRB\n",
    "\n",
    "## <b>I) Prise en main de Keras</b>\n",
    "\n",
    "Keras est une sur-couche de Tensorflow et facilite son utilisation.  \n",
    "Keras et Tensorflow sont principalement dédiées à la programmation de réseaux de neurones convolutifs.  \n",
    "\n",
    "Pour le chargement des données et la séparation en sous-ensembles d'apprentissage et de test, on peut utiliser les fonctions déjà utilisées avec les algorithmes de Machine Learning.\n",
    "\n",
    "### <b>I.1) MLP avec Iris (en Keras)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#chargement des données\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=1)\n",
    "\n",
    "#codage des labels au format des sorties, avec une sortie à 1 et les autres à 0\n",
    "def one_hot_encode_object_array(arr):    \n",
    "    uniques, ids = np.unique(arr, return_inverse=True)\n",
    "    return np_utils.to_categorical(ids, len(uniques))\n",
    "y_train = one_hot_encode_object_array(y_train)\n",
    "y_test = one_hot_encode_object_array(y_test)\n",
    "\n",
    "#définition du modèle\n",
    "model=Sequential()\n",
    "model.add(Dense(20, input_dim=4, activation='relu'))         #couche cachée de 20 neurones\n",
    "model.add(Dense(3, activation='softmax'))                    #couche de sortie (3 neurones de sortie car 3 classes)\n",
    "\n",
    "#affichage de la structure réseau\n",
    "model.summary()           \n",
    "\n",
    "#paramétrage\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "#æpprentissage\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=1, verbose=1, validation_data=(X_test, y_test))\n",
    "\n",
    "#évaluation\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(\"Accuracy = {:.2f}\".format(accuracy))\n",
    "\n",
    "#affichage évolution accuracy\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.title(\"accuracy vs learning epochs on Iris dataset\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.plot(history.history['accuracy'], '--', label='train')\n",
    "plt.plot(history.history['val_accuracy'], label='validation')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>I.2) MLP avec MNIST (en Keras)</b>\n",
    "\n",
    "#### <b>Chargement et mise en forme des données</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "batch_size = 128\n",
    "nb_classes = 10\n",
    "epochs = 15\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "train_samples, image_width, image_height = x_train.shape[0], x_train.shape[1], x_train.shape[2]\n",
    "test_samples = x_test.shape[0]\n",
    "print('train samples: ', train_samples)\n",
    "print('test samples: ', test_samples)\n",
    "print('image width: ', image_width)\n",
    "print('image height: ', image_height)\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], image_width*image_height)\n",
    "x_test = x_test.reshape(x_test.shape[0], image_width*image_height)\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "image_pixels = x_train.shape[1]\n",
    "print('number of pixels: ', image_pixels)\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "print(type(x_train))\n",
    "print(np.shape(x_train))\n",
    "print(np.shape(y_train))\n",
    "print(np.shape(x_test))\n",
    "print(np.shape(y_test))\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Définition du modèle et paramétrage</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#définition du modèle\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation='relu', input_shape=(image_pixels,)))\n",
    "model.add(Dense(nb_classes, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "#paramétrage\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Apprentissage</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Evaluation et affichage historique</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print('Test loss : %.3f' %score[0])\n",
    "print('Test accuracy : %.3f' %score[1])\n",
    "\n",
    "#affichage évolution loss\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.title(\"loss vs learning epochs on MNIST dataset\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.plot(history.history['loss'], '--', label='train')\n",
    "plt.plot(history.history['val_loss'], label='validation')\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "#affichage évolution accuracy\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.title(\"accuracy vs learning epochs on MNIST dataset\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.plot(history.history['accuracy'], '--', label='train')\n",
    "plt.plot(history.history['val_accuracy'], label='validation')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>I.3) MLP avec données GTSRB (en Keras) (Exercice 3.2)</b>\n",
    "\n",
    "Les données GTSRB doivent être chargées depuis un emplacement local.  \n",
    "\n",
    "Appliquer aux données GTSRB le modèle étudié ci-dessus avec les données MNIST.\n",
    "\n",
    "Essayer d'améliorer la précision du réseau de neurones (accuracy). On peut jouer pour ça sur :\n",
    "\n",
    "- la taille des images (actuellement redimensionnées à 32x32)\n",
    "- la structure du réseau de neurones (nombre de couches cachées, nombre de neurones dans les couches)\n",
    "- l'ajout de dropout\n",
    "- la normalisation des données\n",
    "\n",
    "L'objectif est d'atteindre une précision de <b>94%</b> avec un modèle possédant <b>moins de 500000 paramètres</b>.  \n",
    "Les lignes de codes modifiées devront être mise en commentaires et les nouvelles lignes de code ajoutées.\n",
    "\n",
    "Faire une seule modification à la fois, en montrant bien l'amélioration obtenue avec/sans la modification.\n",
    "Donner les explications nécessaires à chaque étape (comme dans les exemples ci-dessus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import *\n",
    "from skimage import color, exposure, transform\n",
    "import cv2\n",
    "import skimage as sk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "\n",
    "nb_classes = 43\n",
    "rows, cols = 32, 32\n",
    "\n",
    "data_path = 'data/GTSRB/Final_Training/Images/'\n",
    "\n",
    "imgs = []\n",
    "labels = []\n",
    "\n",
    "def load_GTSRB():\n",
    "    print(\"Load data...\")\n",
    "    images = []\n",
    "    labels = []\n",
    "    for i in range(nb_classes):\n",
    "        image_path = data_path + '/' + format(i, '05d') + '/'\n",
    "        print(\"chargement répertoire\", image_path)\n",
    "        #cpt = 0\n",
    "        for img in glob.glob(image_path + '*.ppm'):\n",
    "            image = cv2.imread(img)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "            image = (image / 255.0)                            #rescale\n",
    "            image = cv2.resize(image, (rows, cols))            #resize\n",
    "            image = image.reshape(rows*cols)                   #data flattening (aplatissement) \n",
    "            images.append(image)\n",
    "            label = int(image_path.split('/')[-2])\n",
    "            labels.append(label)\n",
    "            #cpt = cpt+1\n",
    "            #if(cpt > 200):\n",
    "            #    break\n",
    "    print('OK')\n",
    "    data = np.array(images, dtype='float32')\n",
    "    Y = np.eye(nb_classes, dtype='uint8')[labels]\n",
    "    return (data, Y)\n",
    "\n",
    "#define CNN\n",
    "def model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, activation='relu', input_shape=(rows*cols,)))\n",
    "    model.add(Dropout(0.15))\n",
    "    model.add(Dense(80, activation='relu'))\n",
    "    model.add(Dropout(0.15))\n",
    "    model.add(Dense(nb_classes, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "#main program\n",
    "X, y = load_GTSRB()\n",
    "\n",
    "#print(np.shape(X))\n",
    "#print(np.shape(y))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "print(np.shape(X_train))\n",
    "print(np.shape(y_train))\n",
    "print(np.shape(X_test))\n",
    "print(np.shape(y_test))\n",
    "\n",
    "model = model()\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'],\n",
    "              optimizer=tensorflow.keras.optimizers.Adam(learning_rate=0.001))\n",
    "\n",
    "history=model.fit(X_train, y_train, batch_size=32, epochs=15, verbose=1, validation_data=(X_test, y_test))\n",
    "\n",
    "#affichage évolution loss\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.title(\"loss vs learning epochs on GTSRB dataset\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.plot(history.history['loss'], '--', label='train')\n",
    "plt.plot(history.history['val_loss'], label='validation')\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "#affichage évolution précision\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.title(\"accuracy vs learning epochs on GTSRB dataset\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.plot(history.history['accuracy'], '--', label='train')\n",
    "plt.plot(history.history['val_accuracy'], label='validation')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>II) Prise en main de Pytorch</b>\n",
    "\n",
    "Pytorch, tout comme Tensorflow/Keras, est une librairie basée sur la manipulation de tenseurs.  \n",
    "Rm : Pour l'instant on ne travaille que sur des images monochromes (même pour les images du dataset, qui sont des images couleurs à la base mais que l'on convertit en niveaux de gris).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>II.1) MLP avec données Iris (en Pytorch)</b>\n",
    "\n",
    "On peut trouver de nombreuses implémentations de classification par MLP des données Iris en Pytorch.  \n",
    "Ces exemples utilisent que des couches de type \"Fully Connected\" (type \"Linear\" dans Pytorch).\n",
    "\n",
    "On se base ici sur celle proposée sur le Github suivant : https://github.com/yangzhangalmo/pytorch-iris\n",
    "\n",
    "#### <b>Importation des librairies nécessaires</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Chargement des données</b>\n",
    "\n",
    "On peut tétlécharger les données Iris au format CSV depuis le lien : https://raw.githubusercontent.com/yangzhangalmo/pytorch-iris/master/dataset/iris.csv  \n",
    "Une fois les données chargées, on les sépare en 2 sous-ensembles apprentissage/test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/yangzhangalmo/pytorch-iris/master/dataset/iris.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load IRIS dataset\n",
    "dataset = pd.read_csv('iris.csv')\n",
    "\n",
    "# transform species to numerics\n",
    "dataset.loc[dataset.species=='Iris-setosa', 'species'] = 0\n",
    "dataset.loc[dataset.species=='Iris-versicolor', 'species'] = 1\n",
    "dataset.loc[dataset.species=='Iris-virginica', 'species'] = 2\n",
    "\n",
    "print(dataset.head())\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(dataset[dataset.columns[0:4]].values,\n",
    "                                                    dataset.species.values, test_size=0.2)\n",
    "\n",
    "# wrap up with Variable in pytorch\n",
    "train_X = Variable(torch.Tensor(train_X).float())\n",
    "test_X = Variable(torch.Tensor(test_X).float())\n",
    "train_y = Variable(torch.Tensor(list(train_y)).long())\n",
    "test_y = Variable(torch.Tensor(list(test_y)).long())\n",
    "\n",
    "print(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Création du réseau de neurones</b>\n",
    "\n",
    "On peut commencer par un réseau très simple, à une seule couche cachée, pour voir déjà ce que ça donne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.h1 = nn.Linear(4, 10)                  \n",
    "        self.out = nn.Linear(10, 3)      \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.h1(x))\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    \n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Affichage de la structure du modèle</b>\n",
    "\n",
    "Il existe une fonction Pytorch analogue à celle de Keras pour afficher la structure du réseau de neurones et ses paramètres.   \n",
    "Mais contrairement à la version de Keras, il faut lui passer la dimension des entrées en paramètres, au format (channels, H, W).  \n",
    "Dans le cas d'Iris, on a un seul channel et il n'y a qu'une seule dimension des données : 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(net, input_size=(1, 4), device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Fonction de coût (\"loss\") et optimiseur (\"optimizer\")</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "#optimizer = torch.optim.SGD(net.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Apprentissage</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m----> 5\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m(train_X)\n\u001b[1;32m      6\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(out, train_y)\n\u001b[1;32m      7\u001b[0m     losses\u001b[38;5;241m.\u001b[39mappend(loss)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'net' is not defined"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    out = net(train_X)\n",
    "    loss = criterion(out, train_y)\n",
    "    losses.append(loss)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'epoch: {epoch:2}  loss: {loss.item():10.8f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_out = net(test_X)\n",
    "_, predict_y = torch.max(predict_out, 1)\n",
    "\n",
    "print(f'accuracy on test set: {accuracy_score(test_y.data, predict_y.data):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Affichage graphique des résultats</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(epochs), losses)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Sauvegarde du modèle, pour utilisation ultérieure</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"model.pt\")\n",
    "print('model saved')\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in net.state_dict():\n",
    "    print(param_tensor, \"\\t\", net.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>II.2) MLP avec données MNIST (en Pytorch)</b>\n",
    "\n",
    "Pour les données MNIST, il existe des fonctions de chargement direct intégrées dans Torchvision.\n",
    "Au premier lancement du programme, les données sont chargées d'Internet. Les fois suivantes, elles sont chargées d'un emplacement local.\n",
    "\n",
    "#### <b>Importation des librairies nécessaires</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSs\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Chargement des données</b>\n",
    "\n",
    "Dans PyTorch, les données sont chargées au moyen d'une fonction \"DataLoader\".    \n",
    "Il faut l'utiliser pour les données d'apprentissage d'une part et pour les données de test d'autre part.  \n",
    "Au moment du chargement des données, on peut leur appliquer un certain nombre de transformations, notamment :\n",
    "\n",
    "- \"Resize\" : redimensionnement des images. Celui-ci est nécessaire si la taille des images est variable ou différente de la taille de la couche d'entrée du réseau.\n",
    "- \"Normalize : normalisation\n",
    "- ...\n",
    "\n",
    "On peut également faire de l'augmentation de données (rotation, translation, miroir, etc) par ce moyen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "\n",
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))      #normalisation pour moyenne nulle et variance de 1\n",
    "        ])\n",
    "\n",
    "train_set = datasets.MNIST(root='.',\n",
    "                                            train=True,            #sous-ensemble d'apprentissage\n",
    "                                            transform=transform,\n",
    "                                            download=True)         #charge d'Internet la première fois\n",
    "\n",
    "test_set = datasets.MNIST(root='.',\n",
    "                                            train=False,           #sous-esnemble de test\n",
    "                                            transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le DataLoader, on définit également un certain nombre d'informations, notamment :\n",
    "- \"batch_size\" : taille du batch, c'est à dire :\n",
    "    - pour les données d'apprentissage, le nombre d'images présentées en entrée du réseau avant adaptation de ses poids par la règle d'apprentissage\n",
    "    - pour les données de test, le nombre d'images présentées en entrée du réseau avant calcul du taux de reconnaissance (\"Accuracy\")\n",
    "- \"shuffle\" : mélange aléatoire des images avant tirage (ou pas)\n",
    "- etc.  \n",
    "\n",
    "Ici, on choisit de mélanger les données d'apprentissage mais pas les données de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_set,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_set,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "\n",
    "\"\"\"\n",
    "#si on veut réduire le dataset\n",
    "list1 = list(range(0, int(len(train_dataset)/10)))\n",
    "list2 = list(range(0, int(len(test_dataset)/10)))\n",
    "\n",
    "train_subset = torch.utils.data.Subset(train_dataset, list1)\n",
    "test_subset = torch.utils.data.Subset(test_dataset, list2)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_subset,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_subset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\"\"\"\n",
    "\n",
    "print(len(train_loader.dataset))\n",
    "print(len(test_loader.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Création du réseau de neurones</b>\n",
    "\n",
    "En première approche, on choisit une structure simple composée de 2 couches convolutives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP_MNIST(\n",
      "  (fc_h1): Linear(in_features=784, out_features=100, bias=True)\n",
      "  (fc_out): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n",
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Linear: 1-1                            78,500\n",
      "├─Linear: 1-2                            1,010\n",
      "=================================================================\n",
      "Total params: 79,510\n",
      "Trainable params: 79,510\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "├─Linear: 1-1                            78,500\n",
       "├─Linear: 1-2                            1,010\n",
       "=================================================================\n",
       "Total params: 79,510\n",
       "Trainable params: 79,510\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im_w, im_h = 28, 28   #taille des images\n",
    "nb_cl = 10            #nombre de classes\n",
    "\n",
    "class MLP_MNIST(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP_MNIST, self).__init__()\n",
    "        self.fc_h1 = nn.Linear(im_w*im_h, 100)\n",
    "        self.fc_out = nn.Linear(100, nb_cl)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, im_w*im_h)\n",
    "        x = F.relu(self.fc_h1(x))\n",
    "        out = self.fc_out(x)\n",
    "        return out\n",
    "\n",
    "model = MLP_MNIST().to(device)      #creation du modele\n",
    "\n",
    "print(model)\n",
    "\n",
    "from torchsummary import summary\n",
    "summary(model, input_size=(1, im_w*im_h), device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Fonction de coût (\"loss\") et optimiseur (\"optimizer\")</b>\n",
    "\n",
    "Pour l'apprentissage, on doit choisir la fonction de coût (loss) à utiliser. Il en existe différents types.\n",
    "Il existe également plusieurs type d'optimiseurs. L'optimiseur est l'algorithme permettant de réduire le loss à chaque itération d'apprentissage.\n",
    "Parmi les paramètres d'apprentissage, on trouve le taux d'apprentissage (learning rate), qui ajuste la quantité de modification des poids du réseau de neurones à chaque itération d'apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "#optimizer = torch.optim.Adadelta(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Fonction d'apprentissage</b>\n",
    "\n",
    "On écrit une fonction d'apprentissage pour les boucles sur les batchs d'images.\n",
    "Rappel de l'algorithme d'apprentissage :  \n",
    "\n",
    "- présentation d'un batch d'images d'apprentissage en entrée du réseau\n",
    "- calcul des sorties correspondantes du réseau\n",
    "- calcul de l'erreur de sortie\n",
    "- modification des poids du réseau de neurones en fonction de cette erreur, de la dernière couche à la première (*)\n",
    "\n",
    "(*)\"rétro-propagation du gradient de l'erreur\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    data_len = len(train_loader.dataset)\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)         # calcul de l'erreur de sortie\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 50 == 0:\n",
    "            print('Train Epoch: {} [{}/{}]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), data_len, loss.item()))\n",
    "        train_loss += loss.item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)          # get the index of the max log-probability\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()  \n",
    "    train_loss /= data_len\n",
    "    accuracy = 100. * correct / len(train_loader.dataset)\n",
    "    print('Train Epoch: {} [{}/{}]\\tLoss: {:.4f}\\tTrain accuracy: {:.4f}'.format(\n",
    "                epoch, batch_idx * len(data), data_len, loss.item(), accuracy))\n",
    "    return train_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Fonction de test</b>\n",
    "\n",
    "La fonction de test réalise l'activation du réseau de neurones (c'est à dire de ses couches, de la première à la dernière), mais cette fois-ci sans appliquer l'apprentissage. Cela permet de comparer sa sortie réelle à la sortie désirée, afin d'évaluer ses performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target)         # calcul de l'erreur\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Test accuracy: ({:.2f}%)\\n'.format(test_loss, accuracy))\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut alors lancer la boucle d'apprentissage sur les époques.  \n",
    "Le fait de faire le test à chaque itération de cette boucle permet d'obtenir le loss et l'accuracy sur les données de test (dans le but de voir leur évolution au fil des itérations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning (please wait)...\n",
      "Train Epoch: 1 [0/60000]\tLoss: 2.368697\n",
      "Train Epoch: 1 [6400/60000]\tLoss: 0.465665\n",
      "Train Epoch: 1 [12800/60000]\tLoss: 0.445150\n",
      "Train Epoch: 1 [19200/60000]\tLoss: 0.303156\n",
      "Train Epoch: 1 [25600/60000]\tLoss: 0.164354\n",
      "Train Epoch: 1 [32000/60000]\tLoss: 0.112437\n",
      "Train Epoch: 1 [38400/60000]\tLoss: 0.205094\n",
      "Train Epoch: 1 [44800/60000]\tLoss: 0.187158\n",
      "Train Epoch: 1 [51200/60000]\tLoss: 0.226939\n",
      "Train Epoch: 1 [57600/60000]\tLoss: 0.343009\n",
      "Train Epoch: 1 [44928/60000]\tLoss: 0.1670\tTrain accuracy: 92.1300\n",
      "\n",
      "Test set: Average loss: 0.0018, Test accuracy: (93.61%)\n",
      "\n",
      "Train Epoch: 2 [0/60000]\tLoss: 0.149987\n",
      "Train Epoch: 2 [6400/60000]\tLoss: 0.147577\n",
      "Train Epoch: 2 [12800/60000]\tLoss: 0.068113\n",
      "Train Epoch: 2 [19200/60000]\tLoss: 0.116346\n",
      "Train Epoch: 2 [25600/60000]\tLoss: 0.261403\n",
      "Train Epoch: 2 [32000/60000]\tLoss: 0.158536\n",
      "Train Epoch: 2 [38400/60000]\tLoss: 0.175573\n",
      "Train Epoch: 2 [44800/60000]\tLoss: 0.163220\n",
      "Train Epoch: 2 [51200/60000]\tLoss: 0.107554\n",
      "Train Epoch: 2 [57600/60000]\tLoss: 0.151294\n",
      "Train Epoch: 2 [44928/60000]\tLoss: 0.1986\tTrain accuracy: 95.4217\n",
      "\n",
      "Test set: Average loss: 0.0013, Test accuracy: (95.50%)\n",
      "\n",
      "Train Epoch: 3 [0/60000]\tLoss: 0.054836\n",
      "Train Epoch: 3 [6400/60000]\tLoss: 0.260761\n",
      "Train Epoch: 3 [12800/60000]\tLoss: 0.078815\n",
      "Train Epoch: 3 [19200/60000]\tLoss: 0.106043\n",
      "Train Epoch: 3 [25600/60000]\tLoss: 0.244953\n",
      "Train Epoch: 3 [32000/60000]\tLoss: 0.309494\n",
      "Train Epoch: 3 [38400/60000]\tLoss: 0.092352\n",
      "Train Epoch: 3 [44800/60000]\tLoss: 0.133673\n",
      "Train Epoch: 3 [51200/60000]\tLoss: 0.061507\n",
      "Train Epoch: 3 [57600/60000]\tLoss: 0.223003\n",
      "Train Epoch: 3 [44928/60000]\tLoss: 0.0893\tTrain accuracy: 95.9433\n",
      "\n",
      "Test set: Average loss: 0.0016, Test accuracy: (94.85%)\n",
      "\n",
      "Train Epoch: 4 [0/60000]\tLoss: 0.137413\n",
      "Train Epoch: 4 [6400/60000]\tLoss: 0.261829\n",
      "Train Epoch: 4 [12800/60000]\tLoss: 0.126631\n",
      "Train Epoch: 4 [19200/60000]\tLoss: 0.154334\n",
      "Train Epoch: 4 [25600/60000]\tLoss: 0.182845\n",
      "Train Epoch: 4 [32000/60000]\tLoss: 0.184259\n",
      "Train Epoch: 4 [38400/60000]\tLoss: 0.136642\n",
      "Train Epoch: 4 [44800/60000]\tLoss: 0.045792\n",
      "Train Epoch: 4 [51200/60000]\tLoss: 0.071953\n",
      "Train Epoch: 4 [57600/60000]\tLoss: 0.025437\n",
      "Train Epoch: 4 [44928/60000]\tLoss: 0.1152\tTrain accuracy: 96.3650\n",
      "\n",
      "Test set: Average loss: 0.0013, Test accuracy: (95.72%)\n",
      "\n",
      "Train Epoch: 5 [0/60000]\tLoss: 0.177770\n",
      "Train Epoch: 5 [6400/60000]\tLoss: 0.111968\n",
      "Train Epoch: 5 [12800/60000]\tLoss: 0.075678\n",
      "Train Epoch: 5 [19200/60000]\tLoss: 0.062291\n",
      "Train Epoch: 5 [25600/60000]\tLoss: 0.187695\n",
      "Train Epoch: 5 [32000/60000]\tLoss: 0.057011\n",
      "Train Epoch: 5 [38400/60000]\tLoss: 0.120698\n",
      "Train Epoch: 5 [44800/60000]\tLoss: 0.095109\n",
      "Train Epoch: 5 [51200/60000]\tLoss: 0.194721\n",
      "Train Epoch: 5 [57600/60000]\tLoss: 0.138117\n",
      "Train Epoch: 5 [44928/60000]\tLoss: 0.0314\tTrain accuracy: 96.5633\n",
      "\n",
      "Test set: Average loss: 0.0012, Test accuracy: (96.48%)\n",
      "\n",
      "Train Epoch: 6 [0/60000]\tLoss: 0.080195\n",
      "Train Epoch: 6 [6400/60000]\tLoss: 0.050008\n",
      "Train Epoch: 6 [12800/60000]\tLoss: 0.006742\n",
      "Train Epoch: 6 [19200/60000]\tLoss: 0.137394\n",
      "Train Epoch: 6 [25600/60000]\tLoss: 0.175334\n",
      "Train Epoch: 6 [32000/60000]\tLoss: 0.201229\n",
      "Train Epoch: 6 [38400/60000]\tLoss: 0.190849\n",
      "Train Epoch: 6 [44800/60000]\tLoss: 0.180297\n",
      "Train Epoch: 6 [51200/60000]\tLoss: 0.157941\n",
      "Train Epoch: 6 [57600/60000]\tLoss: 0.061832\n",
      "Train Epoch: 6 [44928/60000]\tLoss: 0.1185\tTrain accuracy: 96.8733\n",
      "\n",
      "Test set: Average loss: 0.0015, Test accuracy: (95.88%)\n",
      "\n",
      "Train Epoch: 7 [0/60000]\tLoss: 0.077298\n",
      "Train Epoch: 7 [6400/60000]\tLoss: 0.095856\n",
      "Train Epoch: 7 [12800/60000]\tLoss: 0.062389\n",
      "Train Epoch: 7 [19200/60000]\tLoss: 0.120192\n",
      "Train Epoch: 7 [25600/60000]\tLoss: 0.155842\n",
      "Train Epoch: 7 [32000/60000]\tLoss: 0.137776\n",
      "Train Epoch: 7 [38400/60000]\tLoss: 0.138915\n",
      "Train Epoch: 7 [44800/60000]\tLoss: 0.127333\n",
      "Train Epoch: 7 [51200/60000]\tLoss: 0.097194\n",
      "Train Epoch: 7 [57600/60000]\tLoss: 0.080009\n",
      "Train Epoch: 7 [44928/60000]\tLoss: 0.2149\tTrain accuracy: 96.7600\n",
      "\n",
      "Test set: Average loss: 0.0016, Test accuracy: (95.68%)\n",
      "\n",
      "Train Epoch: 8 [0/60000]\tLoss: 0.037625\n",
      "Train Epoch: 8 [6400/60000]\tLoss: 0.104876\n",
      "Train Epoch: 8 [12800/60000]\tLoss: 0.123055\n",
      "Train Epoch: 8 [19200/60000]\tLoss: 0.312935\n",
      "Train Epoch: 8 [25600/60000]\tLoss: 0.058171\n",
      "Train Epoch: 8 [32000/60000]\tLoss: 0.090679\n",
      "Train Epoch: 8 [38400/60000]\tLoss: 0.111957\n",
      "Train Epoch: 8 [44800/60000]\tLoss: 0.278281\n",
      "Train Epoch: 8 [51200/60000]\tLoss: 0.088734\n",
      "Train Epoch: 8 [57600/60000]\tLoss: 0.030382\n",
      "Train Epoch: 8 [44928/60000]\tLoss: 0.4022\tTrain accuracy: 97.0117\n",
      "\n",
      "Test set: Average loss: 0.0015, Test accuracy: (96.34%)\n",
      "\n",
      "Train Epoch: 9 [0/60000]\tLoss: 0.059535\n",
      "Train Epoch: 9 [6400/60000]\tLoss: 0.277341\n",
      "Train Epoch: 9 [12800/60000]\tLoss: 0.087141\n",
      "Train Epoch: 9 [19200/60000]\tLoss: 0.143681\n",
      "Train Epoch: 9 [25600/60000]\tLoss: 0.206453\n",
      "Train Epoch: 9 [32000/60000]\tLoss: 0.165315\n",
      "Train Epoch: 9 [38400/60000]\tLoss: 0.075811\n",
      "Train Epoch: 9 [44800/60000]\tLoss: 0.094519\n",
      "Train Epoch: 9 [51200/60000]\tLoss: 0.086472\n",
      "Train Epoch: 9 [57600/60000]\tLoss: 0.156847\n",
      "Train Epoch: 9 [44928/60000]\tLoss: 0.0462\tTrain accuracy: 96.9867\n",
      "\n",
      "Test set: Average loss: 0.0018, Test accuracy: (95.56%)\n",
      "\n",
      "Train Epoch: 10 [0/60000]\tLoss: 0.036795\n",
      "Train Epoch: 10 [6400/60000]\tLoss: 0.025235\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "\n",
    "test_losses, test_accuracies, train_losses, train_accuracies = [], [], [], []\n",
    "print('Learning (please wait)...')\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss, train_accuracy = train(model, device, train_loader, optimizer, epoch)\n",
    "    test_loss, test_accuracy = test(model, device, test_loader)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Sauvegarde du modèle pour utilisation extérieure</b>\n",
    "\n",
    "On peut sauvegarder le modèle, pour utilisation ultérieure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), \"model_MNIST.pt\")\n",
    "print('model saved')\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Affichage graphique du loss et du taux de reconnaissance sur les données d'apprentissage et de test</b>\n",
    "\n",
    "Il est toujours intéressant d'afficher graphiquement la progression de l'apprentissage, notamment la précision sur les données d'apprentissage et sur les données de test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "\n",
    "plt.title(\"loss vs learning epochs on MNIST dataset\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.plot(train_losses, label='Train')\n",
    "plt.plot(test_losses, label='Test')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.subplot(2, 1, 2)\n",
    "\n",
    "plt.title(\"accuracy vs learning epochs on MNIST dataset\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.plot(train_accuracies, label='Train')\n",
    "plt.plot(test_accuracies, label='Test')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une analyse rapide de ces résultats montre que la précision sur les données de test est très bonne dès les premières époques d'apprentissage. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Utilisation ultérieure : chargement du modèle généré précédemment et inférence</b>\n",
    "\n",
    "Par inférence, on entend la présentation d'une image en entrée du modèle et le calcul de la prédiction de ce dernier.  \n",
    "L'utilisation du réseau de neurones en inférence consiste à charger le fichier-modèle, puis l'application à des images de test.  \n",
    "\n",
    "Remarque : pour être sûr que le modèle présent en mémoire soit celui qui est chargé du fichier, il faut redémarrer le noyau du Notebook.  \n",
    "Dans ce cas, il faut relancer relancer une partie du code défini plus haut. Plus précisément :\n",
    "\n",
    "- Importation des librairies nécessaires\n",
    "- Définition de la structure du réseau de neurones ainsi que son flot d'information\n",
    "- Création d'un réseau de neurones et adaptation au processeur utilisé\n",
    "\n",
    "Pour le test, on peut récupérer une image PNG directement du github suivant :\n",
    "\n",
    "https://github.com/Paperspace/mnist-sample/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp: cannot stat 'example5.png': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!cp example5.png test.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MLP_MNIST' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [11], line 23\u001b[0m\n\u001b[1;32m     19\u001b[0m true_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m     21\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m \n\u001b[0;32m---> 23\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mMLP_MNIST\u001b[49m()\u001b[38;5;241m.\u001b[39mto(device)      \u001b[38;5;66;03m#creation d'un nouvelle instance du modele\u001b[39;00m\n\u001b[1;32m     25\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_MNIST.pth\u001b[39m\u001b[38;5;124m'\u001b[39m))    \u001b[38;5;66;03m# chargement des poids sauvegardés précédemment\u001b[39;00m\n\u001b[1;32m     27\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(path \u001b[38;5;241m+\u001b[39m image_file)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MLP_MNIST' is not defined"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def predict(model, image):\n",
    "    image = image.resize((im_w, im_h))\n",
    "    np_img = np.array(image)                        # conversion en tableau numpy\n",
    "    image_tensor = torch.from_numpy(np_img)         # conversion en tenseur PyTorch\n",
    "    print(image_tensor.shape)\n",
    "    image_tensor = image_tensor.reshape([1, 1, np_img.shape[0], np_img.shape[1]]) # ajout d'une dimension\n",
    "    output = model(image_tensor.float())            # application de l'image en entrée du réseau\n",
    "    #index = output.data.cpu().numpy().argmax()\n",
    "    index = output.data.numpy().argmax()\n",
    "    return index\n",
    "\n",
    "#programme principal\n",
    "path = './'\n",
    "image_file = 'test.png'\n",
    "true_index = 5\n",
    "\n",
    "device = 'cpu' \n",
    "\n",
    "model = MLP_MNIST().to(device)      #creation d'un nouvelle instance du modele\n",
    "\n",
    "model.load_state_dict(torch.load('model_MNIST.pth'))    # chargement des poids sauvegardés précédemment\n",
    "\n",
    "img = Image.open(path + image_file)\n",
    "img = img.convert('L')               #conversion en monochrome\n",
    "index = predict(model, img)\n",
    "print('indice classe estimée : ', index)\n",
    "\n",
    "#affichage du résultat sur l'image\n",
    "classe = [i for i in range(10)]    #nom des classes\n",
    "print('classes : ', classe)\n",
    "if(true_index == index):\n",
    "    res = True\n",
    "else:\n",
    "    res = False\n",
    "plt.title('classe : ' + str(classe[index]) + ' (' + str(res) + ')')\n",
    "plt.axis('off')\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>II.3) MLP avec données GTSRB (en Pytorch) (Exercice 3.3)</b> \n",
    "\n",
    "Idem exercice 3.1 mais en Pytorch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Importation des librairies nécessaires</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSs\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# Device configuration\n",
    "#device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Chargement des données</b>\n",
    "\n",
    "Le système de DataLoader de Pytorch permet de charger les données depuis un emplacement local. Il existe également une fonction \"random_split()\" permettant de séparer un ensemble de données en deux sous-ensembles (apprentissage/test).  \n",
    "La solution la plus simple consiste donc à ne charger que les données d'apprentissage, puis de les séparer en deux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39209\n",
      "31367\n",
      "7842\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "im_w, im_h = 32, 32  #on peut mettre ce qu'on veut comme taille des images, mais garder à peu les proportions d'origine\n",
    "\n",
    "train_dir = 'data/GTSRB/Final_Training/Images/'\n",
    "\n",
    "transform = transforms.Compose([\n",
    "                                transforms.Resize((im_w, im_h)),\n",
    "                                transforms.Grayscale(num_output_channels=1),\n",
    "                                transforms.ToTensor(),   #car la transformation qui suit porte sur des tenseurs\n",
    "                                transforms.Normalize((0.3),(0.26))\n",
    "                               ])\n",
    "\n",
    "full_dataset = datasets.ImageFolder(train_dir, transform=transform)\n",
    "\n",
    "#split dataset for train/test\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "print(len(full_dataset))\n",
    "print(len(train_dataset))\n",
    "print(len(val_dataset))\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compléter cet exemple avec les différentes étapes suivantes (en utilisant des cellules séparées pour faciliter le test) :\n",
    "\n",
    "- définition de la structure du réseau de neurones ainsi que son flot d'information\n",
    "- création d'un réseau de neurones et adaptation au processeur utilisé\n",
    "- paramétrage de l'apprentissage et du test\n",
    "- fonction d'apprentissage\n",
    "- fonction de test\n",
    "- boucle d'apprentissage\n",
    "- affichage graphique du loss et du taux de reconnaissance sur les données d'apprentissage et de test\n",
    "- sauvegarde du modèle\n",
    "- utilisation ultérieure : chargement du modèle généré précédemment et inférence\n",
    "- amélioration de la précision\n",
    "\n",
    "Remarque : dans un premier temps, on utilisera pour le réseau de neurones la structure simple déjà utilisée avec Keras, pour ne pas avoir des durées d'apprentissage trop longues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Définition de la structure du réseau de neurones ainsi que son flot d'information</b>\n",
    "\n",
    "On peut reprendre la structure générale du réseau de neurones déjà utilisé avec MNIST, mais il faut adapter le nombre de classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_cl = 43            #nombre de classes\n",
    "\n",
    "class MLP_GTSRB(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP_GTSRB, self).__init__() \n",
    "        self.conv1=nn.Conv2d(1,6,5)\n",
    "        self.pool=nn.MaxPool2d(2,2)\n",
    "        self.conv2=nn.Conv2d(6,16,5)\n",
    "        #self.fc_h1 = nn.Linear(im_w*im_h, 120)\n",
    "        #self.dropout1 = nn.Dropout(p=0.2)\n",
    "        self.fc_h1 = nn.Linear(16*5*5, 120)\n",
    "        self.fc_h2 = nn.Linear(120, 80)\n",
    "        self.dropout2 = nn.Dropout(p=0.2)\n",
    "        self.fc_out = nn.Linear(80, nb_cl)      \n",
    "    def forward(self, x):\n",
    "        x=self.pool(F.relu(self.conv1(x)))\n",
    "        x=self.pool(F.relu(self.conv2(x)))\n",
    "        #x = x.view(-1, im_w*im_h)\n",
    "        x = x.view(-1, 16*5*5)\n",
    "        x = F.relu(self.fc_h1(x))\n",
    "        #x = self.dropout1(x)\n",
    "        x = F.relu(self.fc_h2(x))\n",
    "        x = self.dropout2(x)\n",
    "        out = self.fc_out(x)\n",
    "        return out\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Création d'un réseau de neurones et adaptation au processeur utilisé</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP_GTSRB(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc_h1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc_h2): Linear(in_features=120, out_features=80, bias=True)\n",
      "  (dropout2): Dropout(p=0.2, inplace=False)\n",
      "  (fc_out): Linear(in_features=80, out_features=43, bias=True)\n",
      ")\n",
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Conv2d: 1-1                            156\n",
      "├─MaxPool2d: 1-2                         --\n",
      "├─Conv2d: 1-3                            2,416\n",
      "├─Linear: 1-4                            48,120\n",
      "├─Linear: 1-5                            9,680\n",
      "├─Dropout: 1-6                           --\n",
      "├─Linear: 1-7                            3,483\n",
      "=================================================================\n",
      "Total params: 63,855\n",
      "Trainable params: 63,855\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "├─Conv2d: 1-1                            156\n",
       "├─MaxPool2d: 1-2                         --\n",
       "├─Conv2d: 1-3                            2,416\n",
       "├─Linear: 1-4                            48,120\n",
       "├─Linear: 1-5                            9,680\n",
       "├─Dropout: 1-6                           --\n",
       "├─Linear: 1-7                            3,483\n",
       "=================================================================\n",
       "Total params: 63,855\n",
       "Trainable params: 63,855\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLP_GTSRB().to('cpu')      #creation du modele\n",
    "\n",
    "print(model)\n",
    "\n",
    "from torchsummary import summary\n",
    "summary(model, input_size=(3, im_w*im_h), device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Paramétrage de l'apprentissage et du test</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "#optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "#optimizer = torch.optim.Adadelta(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Fonction d'apprentissage</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    data_len = len(train_loader.dataset)\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)         # calcul de l'erreur de sortie\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 50 == 0:\n",
    "            print('Train Epoch: {} [{}/{}]\\tLoss: {:.6f}'.format(epoch, batch_idx * len(data), data_len, loss.item()))\n",
    "        train_loss += loss.item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)          # get the index of the max log-probability\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()  \n",
    "    train_loss /= data_len\n",
    "    accuracy = 100. * correct / len(train_loader.dataset)\n",
    "    print('Train Epoch: {} [{}/{}]\\tLoss: {:.4f}\\tTrain accuracy: {:.4f}'.format(\n",
    "                epoch, batch_idx * len(data), data_len, loss.item(), accuracy))\n",
    "    return train_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Fonction de test</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)         # calcul de l'erreur\n",
    "            test_loss += loss.item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Test accuracy: ({:.2f}%)\\n'.format(test_loss, accuracy))\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Boucle d'apprentissage</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning (please wait)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:51] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/31367]\tLoss: 3.781919\n",
      "Train Epoch: 1 [12800/31367]\tLoss: 3.074719\n",
      "Train Epoch: 1 [25600/31367]\tLoss: 2.510497\n",
      "Train Epoch: 1 [16470/31367]\tLoss: 1.9689\tTrain accuracy: 20.7097\n",
      "\n",
      "Test set: Average loss: 0.0070, Test accuracy: (51.47%)\n",
      "\n",
      "Train Epoch: 2 [0/31367]\tLoss: 1.867022\n",
      "Train Epoch: 2 [12800/31367]\tLoss: 1.312973\n",
      "Train Epoch: 2 [25600/31367]\tLoss: 1.070992\n",
      "Train Epoch: 2 [16470/31367]\tLoss: 1.0094\tTrain accuracy: 60.8825\n",
      "\n",
      "Test set: Average loss: 0.0031, Test accuracy: (78.67%)\n",
      "\n",
      "Train Epoch: 3 [0/31367]\tLoss: 0.825737\n",
      "Train Epoch: 3 [12800/31367]\tLoss: 0.836663\n",
      "Train Epoch: 3 [25600/31367]\tLoss: 0.643429\n",
      "Train Epoch: 3 [16470/31367]\tLoss: 0.7157\tTrain accuracy: 77.6740\n",
      "\n",
      "Test set: Average loss: 0.0019, Test accuracy: (86.88%)\n",
      "\n",
      "Train Epoch: 4 [0/31367]\tLoss: 0.593520\n",
      "Train Epoch: 4 [12800/31367]\tLoss: 0.510677\n",
      "Train Epoch: 4 [25600/31367]\tLoss: 0.533782\n",
      "Train Epoch: 4 [16470/31367]\tLoss: 0.4671\tTrain accuracy: 85.0544\n",
      "\n",
      "Test set: Average loss: 0.0013, Test accuracy: (91.15%)\n",
      "\n",
      "Train Epoch: 5 [0/31367]\tLoss: 0.395452\n",
      "Train Epoch: 5 [12800/31367]\tLoss: 0.459441\n",
      "Train Epoch: 5 [25600/31367]\tLoss: 0.430183\n",
      "Train Epoch: 5 [16470/31367]\tLoss: 0.4651\tTrain accuracy: 88.9884\n",
      "\n",
      "Test set: Average loss: 0.0011, Test accuracy: (92.48%)\n",
      "\n",
      "Train Epoch: 6 [0/31367]\tLoss: 0.365992\n",
      "Train Epoch: 6 [12800/31367]\tLoss: 0.337891\n",
      "Train Epoch: 6 [25600/31367]\tLoss: 0.227155\n",
      "Train Epoch: 6 [16470/31367]\tLoss: 0.2350\tTrain accuracy: 91.3763\n",
      "\n",
      "Test set: Average loss: 0.0008, Test accuracy: (94.75%)\n",
      "\n",
      "Train Epoch: 7 [0/31367]\tLoss: 0.220735\n",
      "Train Epoch: 7 [12800/31367]\tLoss: 0.303222\n",
      "Train Epoch: 7 [25600/31367]\tLoss: 0.363933\n",
      "Train Epoch: 7 [16470/31367]\tLoss: 0.1998\tTrain accuracy: 93.0500\n",
      "\n",
      "Test set: Average loss: 0.0008, Test accuracy: (94.54%)\n",
      "\n",
      "Train Epoch: 8 [0/31367]\tLoss: 0.270277\n",
      "Train Epoch: 8 [12800/31367]\tLoss: 0.251318\n",
      "Train Epoch: 8 [25600/31367]\tLoss: 0.180403\n",
      "Train Epoch: 8 [16470/31367]\tLoss: 0.2677\tTrain accuracy: 94.2169\n",
      "\n",
      "Test set: Average loss: 0.0006, Test accuracy: (96.14%)\n",
      "\n",
      "Train Epoch: 9 [0/31367]\tLoss: 0.213060\n",
      "Train Epoch: 9 [12800/31367]\tLoss: 0.204237\n",
      "Train Epoch: 9 [25600/31367]\tLoss: 0.168005\n",
      "Train Epoch: 9 [16470/31367]\tLoss: 0.2446\tTrain accuracy: 95.1159\n",
      "\n",
      "Test set: Average loss: 0.0005, Test accuracy: (97.02%)\n",
      "\n",
      "Train Epoch: 10 [0/31367]\tLoss: 0.165992\n",
      "Train Epoch: 10 [12800/31367]\tLoss: 0.134266\n",
      "Train Epoch: 10 [25600/31367]\tLoss: 0.213734\n",
      "Train Epoch: 10 [16470/31367]\tLoss: 0.1483\tTrain accuracy: 95.8364\n",
      "\n",
      "Test set: Average loss: 0.0005, Test accuracy: (97.12%)\n",
      "\n",
      "Train Epoch: 11 [0/31367]\tLoss: 0.147440\n",
      "Train Epoch: 11 [12800/31367]\tLoss: 0.078592\n",
      "Train Epoch: 11 [25600/31367]\tLoss: 0.164386\n",
      "Train Epoch: 11 [16470/31367]\tLoss: 0.2210\tTrain accuracy: 96.1711\n",
      "\n",
      "Test set: Average loss: 0.0004, Test accuracy: (97.49%)\n",
      "\n",
      "Train Epoch: 12 [0/31367]\tLoss: 0.127749\n",
      "Train Epoch: 12 [12800/31367]\tLoss: 0.087039\n",
      "Train Epoch: 12 [25600/31367]\tLoss: 0.135110\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "\n",
    "test_losses, test_accuracies, train_losses, train_accuracies = [], [], [], []\n",
    "print('Learning (please wait)...')\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss, train_accuracy = train(model, 'cpu', train_loader, optimizer, epoch)\n",
    "    test_loss, test_accuracy = test(model, 'cpu', test_loader)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Affichage graphique du loss et du taux de reconnaissance sur les données d'apprentissage et de test</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "\n",
    "plt.title(\"loss vs learning epochs on GTSRB dataset\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.plot(train_losses, label='Train')\n",
    "plt.plot(test_losses, label='Test')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.subplot(2, 1, 2)\n",
    "\n",
    "plt.title(\"accuracy vs learning epochs on GTSRB dataset\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.plot(train_accuracies, label='Train')\n",
    "plt.plot(test_accuracies, label='Test')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Sauvegarde du modèle</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "conv1.weight \t torch.Size([6, 3, 5, 5])\n",
      "conv1.bias \t torch.Size([6])\n",
      "conv2.weight \t torch.Size([16, 6, 5, 5])\n",
      "conv2.bias \t torch.Size([16])\n",
      "fc_h1.weight \t torch.Size([120, 400])\n",
      "fc_h1.bias \t torch.Size([120])\n",
      "fc_h2.weight \t torch.Size([80, 120])\n",
      "fc_h2.bias \t torch.Size([80])\n",
      "fc_out.weight \t torch.Size([43, 80])\n",
      "fc_out.bias \t torch.Size([43])\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"model_GTSRB.pt\")            # Save the model checkpoint\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce format de sauvegarde comporte les poids du réseau de neurones, mais également les paramètres d'apprentissage, les valeurs du loss, etc.\n",
    "Ce fichier peut donc servir de base à la reprise de l'apprentissage. On parle alors de checkpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Utilisation ultérieure : chargement du modèle généré précédemment et inférence</b>\n",
    "\n",
    "Par inférence, on entend la présentation d'une image en entrée du modèle et le calcul de la prédiction de ce dernier.  \n",
    "L'utilisation du réseau de neurones en inférence consiste à charger le fichier-modèle, puis à l'appliquer à des images de test.  \n",
    "Remarque : pour être sûr que le modèle présent en mémoire soit celui qui est chargé du fichier, il faut redémarrer le noyau du Notebook.  \n",
    "Dans ce cas, il faut relancer une partie du code défini plus haut. Plus précisément :\n",
    "- Importation des librairies nécessaires\n",
    "- Définition de la structure du réseau de neurones ainsi que son flot d'information\n",
    "- Création d'un réseau de neurones et adaptation au processeur utilisé\n",
    "\n",
    "On affichera l'image avec pour titre la classe réelle et la classe estimée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp example5.png test.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut également vouloir faire un test sur toute la base de test.\n",
    "Dans ce cas il faut exécuter en plus :\n",
    "\n",
    "- le chargement des données\n",
    "- la fonction de test\n",
    "- le paramétrage du test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def predict(model, image):\n",
    "    image = image.resize((im_w, im_h))\n",
    "    np_img = np.array(image)                        # conversion en tableau numpy\n",
    "    image_tensor = torch.from_numpy(np_img)         # conversion en tenseur PyTorch\n",
    "    print(image_tensor.shape)\n",
    "    image_tensor = image_tensor.reshape([1, 1, np_img.shape[0], np_img.shape[1]]) # ajout d'une dimension\n",
    "    output = model(image_tensor.float())            # application de l'image en entrée du réseau\n",
    "    #index = output.data.cpu().numpy().argmax()\n",
    "    index = output.data.numpy().argmax()\n",
    "    return index\n",
    "\n",
    "#programme principal\n",
    "path = './'\n",
    "image_file = 'test.png'\n",
    "true_index = 5\n",
    "\n",
    "device = 'cpu' \n",
    "\n",
    "model = MLP_MNIST().to(device)      #creation d'un nouvelle instance du modele\n",
    "\n",
    "model.load_state_dict(torch.load('model_GTSRB.pth'))    # chargement des poids sauvegardés précédemment\n",
    "\n",
    "img = Image.open(path + image_file)\n",
    "img = img.convert('L')               #conversion en monochrome\n",
    "index = predict(model, img)\n",
    "print('indice classe estimée : ', index)\n",
    "\n",
    "#affichage du résultat sur l'image\n",
    "classe = [i for i in range(10)]    #nom des classes\n",
    "print('classes : ', classe)\n",
    "if(true_index == index):\n",
    "    res = True\n",
    "else:\n",
    "    res = False\n",
    "plt.title('classe : ' + str(classe[index]) + ' (' + str(res) + ')')\n",
    "plt.axis('off')\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Amélioration de la précision</b>\n",
    "\n",
    "Pour essayer d'améliorer la précision du réseau de neurones (accuracy), on pourra jouer sur :\n",
    "\n",
    "- la taille des images (actuellement redimensionnées à 32x32)\n",
    "- la structure du réseau de neurones (nombre de couches cachées, nombre de neurones dans les couches)\n",
    "- l'ajout de dropout\n",
    "- la normalisation des données\n",
    "\n",
    "L'objectif est d'atteindre une précision de <b>94%</b> avec un modèle possédant <b>moins de 500000 paramètres</b>.  \n",
    "On pourra ne donner que les modifications dans la (ou les) cellule(s) ci-dessous ; il faudra alors relancer les cellules adéquates ci-dessus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Références</b>\n",
    "\n",
    "- Image processing with numpy :\n",
    "https://pythoninformer.com/python-libraries/numpy/numpy-and-images/\n",
    "\n",
    "- How to Train an Image Classifier in PyTorch and use it to Perform Basic Inference on Single Images :\n",
    "https://towardsdatascience.com/how-to-train-an-image-classifier-in-pytorch-and-use-it-to-perform-basic-inference-on-single-images-99465a1e9bf5\n",
    "\n",
    "- How To Save and Load Model In PyTorch With A Complete Example :\n",
    "https://towardsdatascience.com/how-to-save-and-load-a-model-in-pytorch-with-a-complete-example-c2920e617dee\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Format de la Cellule Texte Brut",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
