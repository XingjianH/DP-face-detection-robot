{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>4. Classification par réseaux de neurones convolutifs</b>\n",
    "## <b>Partie 1 : Petits réseaux de neurones personnalisés</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>4.1 Avec Keras (Exercice 4.1)</b>\n",
    "\n",
    "On peut reprendre l'exemple complet de classification des données GTSRB avec un MLP.\n",
    "Adapter le programme suivant pour obtenir une accuracy >99,5% avec un réseau de neurones convolutif comportant moins de 500000 paramètres.\n",
    "On pourra jouer sur :\n",
    "- le nombre de paramètres\n",
    "- la taille des images\n",
    "- le \"rescale\" des images\n",
    "- la taille des batchs\n",
    "- le dropout\n",
    "- la batch-normalisation\n",
    "- l'augmentation de la base d'apprentissage\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import *\n",
    "from skimage import color, exposure, transform\n",
    "import cv2\n",
    "import skimage as sk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "\n",
    "nb_classes = 43\n",
    "rows, cols = 32, 32\n",
    "\n",
    "data_path = 'data/GTSRB/Final_Training/Images/'\n",
    "\n",
    "imgs = []\n",
    "labels = []\n",
    "\n",
    "def load_GTSRB():\n",
    "    print(\"Load data...\")\n",
    "    images = []\n",
    "    labels = []\n",
    "    for i in range(nb_classes):\n",
    "        image_path = data_path + '/' + format(i, '05d') + '/'\n",
    "        print(\"chargement répertoire\", image_path)\n",
    "        #cpt = 0\n",
    "        for img in glob.glob(image_path + '*.ppm'):\n",
    "            image = cv2.imread(img)\n",
    "            #image = (image / 255.0)                            #rescale\n",
    "            image = cv2.resize(image, (rows, cols))            #resize\n",
    "            images.append(image)\n",
    "            label = int(image_path.split('/')[-2])\n",
    "            labels.append(label)\n",
    "    print('OK')\n",
    "    data = np.array(images, dtype='float32')\n",
    "    Y = np.eye(nb_classes, dtype='uint8')[labels]\n",
    "    return (data, Y)\n",
    "\n",
    "def model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters=8, kernel_size=(3, 3), padding=\"same\", activation=\"relu\",\n",
    "                                                                    input_shape=(rows, cols, 3)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(nb_classes, activation='softmax'))\n",
    "    model.add(Dropout(0.2))\n",
    "    return model\n",
    "\n",
    "#main program\n",
    "X, y = load_GTSRB()\n",
    "\n",
    "#print(np.shape(X))\n",
    "#print(np.shape(y))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "print(np.shape(X_train))\n",
    "print(np.shape(y_train))\n",
    "print(np.shape(X_test))\n",
    "print(np.shape(y_test))\n",
    "\n",
    "model = model()\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'],\n",
    "              optimizer=tensorflow.keras.optimizers.Adam(learning_rate=0.001))\n",
    "\n",
    "history=model.fit(X_train, y_train, batch_size=32, epochs=20, verbose=1, validation_data=(X_test, y_test))\n",
    "\n",
    "#affichage évolution loss\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.title(\"loss vs learning epochs on GTSRB dataset\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.plot(history.history['loss'], '--', label='train')\n",
    "plt.plot(history.history['val_loss'], label='validation')\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "#affichage évolution précision\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.title(\"accuracy vs learning epochs on GTSRB dataset\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.plot(history.history['accuracy'], '--', label='train')\n",
    "plt.plot(history.history['val_accuracy'], label='validation')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Rappels sur les tableaux</b>\n",
    "\n",
    "#### <b>Tableaux Python</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import *\n",
    "from skimage import color, exposure, transform\n",
    "import cv2\n",
    "import skimage as sk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "a = [[1., 2, 3], [4, 5, 6]]\n",
    "print(a)\n",
    "b= [[[1., 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]]\n",
    "print(b)\n",
    "print(type(a))\n",
    "print(type(b))\n",
    "print(a[0])\n",
    "print(a[1])\n",
    "print(a[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "a = [[1., 2, 3], [4, 5, 6]]\n",
    "print(a)\n",
    "b= [[[1., 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]]\n",
    "print(b)\n",
    "print(type(a))\n",
    "print(type(b))\n",
    "print(a[0])\n",
    "print(a[1])\n",
    "print(a[1][0])\n",
    "print(shape(a))\n",
    "print(shape(b))\n",
    "print(len(a))\n",
    "print(len(b))\n",
    "print(len(a[0]))\n",
    "print(len(b[0]))\n",
    "print(len(b[1]))\n",
    "print(len(b[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Tableaux Numpy</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "print(a)\n",
    "b= np.array([[[1., 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])\n",
    "print(b)\n",
    "print(type(a))\n",
    "print(type(b))\n",
    "print(a.dtype)\n",
    "print(b.dtype)\n",
    "print(a.shape)\n",
    "print(b.shape)\n",
    "print(a.size)\n",
    "print(b.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On constate que le tableau a est de type entier (64 bits) car tous ses éléments sont des entiers. Par contre, la tableau b est de type flottant car il y a au moins un élément de type flottant. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Manipulation de tenseurs</b>\n",
    "\n",
    "Les librairies de deep-learning comme Tensorflow et Pytorch sont basées sur le traitement de tenseurs.  \n",
    "Un tenseur est un tableau dont la dimension peut être quelconque.  \n",
    "Notamment :\n",
    "- un nombre (=scalaire) est un tableau de dimension 0 (tenseur de dimension 0), \n",
    "- un vecteur est un tableau de dimension 1 (tenseur de dimension 1), \n",
    "- une matrice est un tableau de dimension 2 (tenseur de dimension 2), etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "t0 = torch.tensor(1.23) #tenseur de dimension 0\n",
    "print(t0)\n",
    "print(t0.ndim)\n",
    "print(t0.shape)\n",
    "print(type(t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.tensor([1, 2, 3]) #tenseur de dimension 1\n",
    "print(t1)\n",
    "print(t1.ndim)\n",
    "print(t1.shape)\n",
    "print(type(t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = torch.tensor([[1., 2, 3], [4, 5, 6]]) #tenseur de dimension 2\n",
    "print(t2)\n",
    "print(t2.ndim)\n",
    "print(t2.shape)\n",
    "print(type(t2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t3 = torch.tensor([[[1., 2], [3, 4], [5, 6]], [[7, 8], [9, 10], [11, 12]], [[13, 14], [15, 16], [17, 18]], [[19, 20], [21, 22], [23, 24]]]) #tenseur de dimension 3\n",
    "print(t3)\n",
    "print(t3.ndim)\n",
    "print(t3.shape)\n",
    "print(type(t3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chaque dimension d'un tenseur possède une taille, qu'on peut afficher séparemment :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t3.shape[0])\n",
    "print(t3.shape[1])\n",
    "print(t3.shape[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Lien avec les images</b>\n",
    "\n",
    "On peut charger une image et la comparer avec les tenseurs étudiés ci-dessus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = plt.imread(\"data/GTSRB/Final_Training/Images/00000/00000_00001.ppm\")\n",
    "\n",
    "plt.imshow(img)\n",
    "\n",
    "print(type(img))\n",
    "print(img.shape)\n",
    "print(img.dtype)\n",
    "print(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On constate que la fonction de chargement de Matplotlib a généré un tableau Numpy, de type entier non signé 8 bits.\n",
    "\n",
    "Pour pouvoir utiliser des images avec les réseaux de neurones de Pytorch, il va falloir les convertir en tenseurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.from_numpy(img) # conversion en tenseur PyTorch\n",
    "print(type(x))\n",
    "print(x.ndim)\n",
    "print(x.dtype)\n",
    "print(x.shape)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On constate que l'image convertie en tenseur a la même structure que le tenseur de dimension 3 étudié plus haut.\n",
    "Cette image, composée de 3 plans couleurs (R, G, B) est donc un tenseur de dimension 3.\n",
    "\n",
    "On peut remarquer que nos tenseurs étaient de type float, alors que celui correspondant à l'image est du type uint8 : entier 8 bits non-signé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Couche de neurones convolutifs</b>\n",
    "\n",
    "Dans un réseau de neurones convolutifs, les couches sont constituées de feature maps.\n",
    "Dans le cas particulier de la 1ère couche du réseau (celle qui reçoit les images d'entrée), les feature maps d'entrée sont constituées par les F canaux des images : F=3 pour les images couleur, F=1 dans le cas monochrome.  \n",
    "\n",
    "De plus, la 1ère couche de neurones ne prend pas en entrée une seule image, mais un ensemble de B images (un \"Batch\"). Ceci parce que l'algorithme d'apprentissage s'applique à des paquets d'images et pas a des images isolées (si l'on veut appliquer l'apprentissage à chaque présentation d'une seule images, il faut utiliser un batch de 1).\n",
    "\n",
    "Dans PyTorch, les entrées des couches de convolution sont donc constituées par des tenseurs de dimension BxFxHxL, avec :\n",
    "- B taille du batch\n",
    "- F nombre de features maps\n",
    "- H hauteur des images\n",
    "- L largeur des images\n",
    "\n",
    "Avant d'appliquer notre image en entrée d'une couche convolutive, revenons sur ses dimensions :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec notre notation, ses dimensions sont HxLxF.\n",
    "Il faut donc d'abord modifier l'ordre de ces dimensions, à savoir ramener la 3e dimension en 1ère position. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.permute(2,0,1)      #nouvel ordre des dimensions (ici, la dernière vient en première position)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mais ça n'est pas suffisant. Les couches convolutives prennent en entrée des groupes d'images, qu'on appelle des \"batchs\". Il faut donc ajouter la dimension du batch. Comme on a une seule image ici, on prend B=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.shape[0])\n",
    "print(x.shape[1])\n",
    "print(x.shape[2])\n",
    "\n",
    "x = x.reshape([1, x.shape[0], x.shape[1], x.shape[2]]) # add a dimension for batching\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut alors l'appliquer en entrée d'une couche de neurones convolutifs.\n",
    "\n",
    "Pour créer une couche convolutive, il faut lui spécifier au moins les éléments suivants :\n",
    "- le nombre de canaux d'entrée, \n",
    "- le nombre de canaux de sortie, \n",
    "- la taille du noyau de convolution (celui-ci étant carré, une taille de 3 signifiera, par exemple, un noyau de dimension 3x3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print('x shape : ', x.shape)\n",
    "conv = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3)\n",
    "y = conv(x.float())\n",
    "print('y shape : ', y.shape)\n",
    "y = y.detach().numpy() # convert the result into numpy\n",
    "print('y shape : ', y.shape)\n",
    "y = y[0]               # remove the dimension for batching\n",
    "print('y shape : ', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On constate que la sortie de la couche est bien composée de 8 feature-maps, de taille 28x28 du fait de l'effet de bord dû à la convolution.\n",
    "\n",
    "On peut afficher graphiquement les sorties des feature-maps :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the result to [0, 1] for plotting\n",
    "y_max = np.max(y)\n",
    "y_min = np.min(y)\n",
    "img_after_conv = y - y_min / (y_max - y_min)\n",
    "img_after_conv.shape\n",
    "#print(img_after_conv.shape)\n",
    "plt.figure(figsize=(14,4))\n",
    "plt.imshow(img_after_conv)\n",
    "for i in range(8):\n",
    "    plt.subplot(1, 8, i+1)\n",
    "    plt.imshow(img_after_conv[i])\n",
    "#rm : autre façon de faire :\n",
    "#for i, img in enumerate(img_after_conv):\n",
    "#    plt.subplot(1, 8, i+1)\n",
    "#    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'interprétation de ces images n'a pas beaucoup de sens dans la mesure où les poids des neurones ont des valeurs aléatoires.\n",
    "\n",
    "La foncntion Conv2d() comporte un argument suplémentaire \"stride\", correspondant au décalage des convolutions.\n",
    "On observe ce qui se passe avec un stride de 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3, stride=2)\n",
    "y = conv(x.float())\n",
    "print('y shape : ', y.shape)\n",
    "y = y.detach().numpy() # convert the result into numpy\n",
    "print('y shape : ', y.shape)\n",
    "y = y[0]               # remove the dimension for batching\n",
    "print('y shape : ', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On constate que la taille des couches a été divisée par 2 (ce qui est normal puisque les convolutions ont été décalées de 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the result to [0, 1] for plotting\n",
    "y_max = np.max(y)\n",
    "y_min = np.min(y)\n",
    "img_after_conv = y - y_min / (y_max - y_min)\n",
    "img_after_conv.shape\n",
    "\n",
    "plt.figure(figsize=(14,4))\n",
    "for i in range(8):\n",
    "    plt.subplot(1, 8, i+1)\n",
    "    plt.imshow(img_after_conv[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit bien que la résolution des sorties des couches a été divisée par 2.\n",
    "\n",
    "\n",
    "### <b>Couche de max-pooling</b>\n",
    "\n",
    "Les couches de max-pooling sont utilisées pour réduire la résolution des sorties des couches de convolution.\n",
    "Cette réduction est quantifiée par le paramètre \"kernel_size\".\n",
    "La plupart du temps, cette taille est prise égale à 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "y = conv(x.float())\n",
    "z = pool(y)\n",
    "\n",
    "print('y shape : ', y.shape)\n",
    "print('z shape : ', z.shape)\n",
    "z = z.detach().numpy() # convert the result into numpy\n",
    "print('y shape : ', z.shape)\n",
    "z = z[0]               # remove the dimension for batching\n",
    "print('z shape : ', z.shape)\n",
    "\n",
    "# normalize the result to [0, 1] for plotting\n",
    "z_max = np.max(z)\n",
    "z_min = np.min(z)\n",
    "img_after_conv = z - z_min / (z_max - z_min)\n",
    "img_after_conv.shape\n",
    "\n",
    "plt.figure(figsize=(14,4))\n",
    "for i in range(8):\n",
    "    plt.subplot(1, 8, i+1)\n",
    "    plt.imshow(img_after_conv[i])\n",
    "    \n",
    "list(pool.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On observe que la résolution des sortie de la couche de convolution précédente a encore été divisée par 2.\n",
    "\n",
    "### <b>Couche complètement connectée</b>\n",
    "\n",
    "Les couches complètement connectées (fully-connected) sont utilisées en sortie d'un réseau de neurones dédié à la classification.\n",
    "Remarque : les réseaux de neurones de type MLP (Multi-Layer Perceptron) sont composés uniquement de couches complètement connectées.\n",
    "\n",
    "Dans Pytorch, elles sont appelées \"Linear\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fully-connected layer between a lower layer of size 100, and a higher layer of size 30\n",
    "fc = nn.Linear(100, 30)\n",
    "\n",
    "x = torch.randn(100) # create a tensor of shape [100]\n",
    "y = fc(x)            # apply the fully conected layer `fc` to x\n",
    "print(\"y.shape :\", y.shape)\n",
    "\n",
    "fc_params = list(fc.parameters())\n",
    "print(\"len(fc_params) :\", len(fc_params))\n",
    "print(\"Weights :\", fc_params[0].shape)\n",
    "print(\"Biases :\", fc_params[1].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Chargement d'un dataset depuis un emplacement local</b>\n",
    "\n",
    "Pour cette partie on va utiliser un dataset existant, mais faire comme si il s'agissait de nos propres images. \n",
    "\n",
    "On peut utiliser pour ça le dataset des images de panneaux de signalisation GTSRB (German Trafic Sign Recognition Benchmark).\n",
    "\n",
    "Rappel de la commande de téléchargement (/!\\ ne pas utiliser sur le serveur ROOC, le dataset est déjà disponible dans data/GTSRB) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://sid.erda.dk/public/archives/daaeac0d7ce1152aea9b61d9f1e19370/GTSRB_Final_Training_Images.zip\n",
    "#!unzip GTSRB_Final_Training_Images.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans PyTorch, les données sont chargées au moyen d'une fonction \"DataLoader\". Les différentes étapes sont les suivantes :\n",
    "- des transformations peuvent être appliquées aux images au moyen de la fonction \"Compose\" (de torchvision)\n",
    "- ces transformations sont appliquées au moyen de la fonction \"ImageFolder\" (également de torchvision)\n",
    "- les données sont chargées au moyen de la fonction \"DataLoader\"\n",
    "\n",
    "La conversion des images en tenseurs peut se faire au niveau de ces transformations.\n",
    "Dans le cas des données GTSRB, une autre transormation est indispensable : le redimensionnement des images à une taille unique (fonction \"resize\"). En effet, les images de ce dataset sont de taille variable, or la taille de la couche d'entrée du réseau de neurones est fixe. On choisira ici une taille de 50x50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "im_w, im_h = 50, 50  #on peut mettre ce qu'on veut comme taille des images, mais garder à peu les proportions d'origine\n",
    "transf = transforms.Compose([\n",
    "                               transforms.Resize((im_w, im_h)),\n",
    "                               transforms.ToTensor()       #conversion en tenseur\n",
    "                            ])\n",
    "train_dataset=datasets.ImageFolder(\"data/GTSRB/Final_Training/Images/\", transform=transf)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "\n",
    "#affichage des 10 1ères images\n",
    "plt.figure(figsize=(20,4))\n",
    "for i in range(10):\n",
    "    plt.subplot(1, 11, i+1)\n",
    "    torchimage = train_dataset[i][0]\n",
    "    npimage = torchimage.permute(1, 2, 0)\n",
    "    plt.imshow(npimage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque que les images ont maintenant bien toutes la même taille."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Exemple complet de classification d'images</b>\n",
    "\n",
    "\n",
    "Les différentes étapes sont les suivantes :\n",
    "- Chargement des données d'apprentissage et de test à partir de 2 répertoires data/train et data/test\n",
    "- Redimensionnement des images\n",
    "- Apprentissage\n",
    "- Sauvegarde du modèle entraîné\n",
    "\n",
    "On commence avec les données MNIST car il existe des fonctions de chargement des données intégrées dans Torchvision.  \n",
    "\n",
    "Rm : l'utilisation des données Iris n'a pas vraiment de sens avec les réseaux de neurones convolutifs, car elles ne sont pas en 2 dimensions. \n",
    "\n",
    "\n",
    "### <b>Données MNIST</b>\n",
    "\n",
    "#### <b>Chargement des librairies utiles et détection de présence d'un GPU</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSs\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Chargement des données</b>\n",
    "\n",
    "Il faut créer un DataLoader pour les données d'apprentissage et un autre pour les données de test..    \n",
    "Le redimensionnement des images n'est pas utile ici, car les images sont déjà toutes de la même taille (28x28 pixels). \n",
    "La seule transformation obligatoire est donc la conversion des images en tenseurs.\n",
    "\n",
    "Les autres transformations qui pourraient être utiles sont :\n",
    "\n",
    "- \"Normalize\" : normalisation des données pour avoir une moyenne nulle et un écart-type égal à 1\n",
    "- rotation, translation, changement d'échelle, etc : qui permettent de faire de l'augmentation de données\n",
    "- ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Normalisation des données</b>\n",
    "\n",
    "Les données sont souvent normalisées pour avoir une moyenne nulle et un écart-type (\"standard deviation\") égal à 1.  \n",
    "La normalisation peut accélerer l'apprentissage et améliorer ses résultats.  \n",
    "Dans le cas d'un ensemble de valeurs dont la moyenne est M et l'écart-type sigma, cette normalisation consiste à remplacer chaque valeur par :\n",
    "\n",
    "valeur = (valeur – M) / sigma\n",
    "\n",
    "Regardons sur un petit tableau l'effet de cette normalisation :  \n",
    "(source : https://inside-machinelearning.com/en/why-and-how-to-normalize-data-object-detection-on-image-in-pytorch-part-1/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moyenne :  24.5\n",
      "écart type :  19.102792117035317\n",
      "[ 1.85836708 -0.81139971  0.65435461 -0.54965787 -0.07852255 -1.07314155]\n",
      "moyenne après normalisation :  0.0\n",
      "écart type après normalisation :  1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "l = [60, 9, 37, 14, 23, 4]\n",
    "\n",
    "print('moyenne : ', np.mean(l))\n",
    "print('écart type : ', np.std(l))\n",
    "\n",
    "l_norm = np.zeros(len(l))\n",
    "for i in range(len(l)):\n",
    "    l_norm[i] = (l[i] - np.mean(l)) / np.std(l)\n",
    "\n",
    "#rm : forme compacte :\n",
    "#l_norm = [(element - np.mean(l)) / np.std(l) for element in l]\n",
    "\n",
    "print(l_norm)\n",
    "print('moyenne après normalisation : ', np.mean(l_norm))\n",
    "print('écart type après normalisation : ', np.std(l_norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De la même façon, dans le cas d'un ensemble de vecteurs, chaque élément des vecteurs va être remplacé par sa valeur normalisée, calculée à partir de toutes ses valeurs sur l'ensemble des vecteurs.  \n",
    "Dans le cas des images, chaque pixel est normalisé en fonction de ses valeurs sur l'ensemble du jeu de données.  \n",
    "Pour connaître la moyenne et la variance des données MNIST, on les charge d'abord sans transformation, pour pouvoir calculer cette moyenne (\"mean\") et cette variance (\"std\" pour \"standard deviation\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.MNIST(root='./my_data', \n",
    "                                           train=True, \n",
    "                                           download=True)     #chargement d'internet, la première fois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min Pixel Value: 0 \n",
      "Max Pixel Value: 255\n",
      "Mean Pixel Value 33.31842041015625 \n",
      "Pixel Values Std: 78.56748962402344\n",
      "Scaled Mean Pixel Value 0.13066047430038452 \n",
      "Scaled Pixel Values Std: 0.30810779333114624\n"
     ]
    }
   ],
   "source": [
    "print('Min Pixel Value: {} \\nMax Pixel Value: {}'.format(train_dataset.data.min(), train_dataset.data.max()))\n",
    "print('Mean Pixel Value {} \\nPixel Values Std: {}'.format(train_dataset.data.float().mean(), train_dataset.data.float().std()))\n",
    "print('Scaled Mean Pixel Value {} \\nScaled Pixel Values Std: {}'.format(train_dataset.data.float().mean() / 255, train_dataset.data.float().std() / 255))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On recommence le chargement, mais cette fois-ci en ajoutant la normalisation. \n",
    "(On efface d'abord le répertoire des données pour que le rechargement depuis internet se produise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r my_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./my_data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3ee3665baf24a20b72f5ba0c30db6a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./my_data/MNIST/raw/train-images-idx3-ubyte.gz to ./my_data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./my_data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b8f72586cef40a988a353f46998e6fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./my_data/MNIST/raw/train-labels-idx1-ubyte.gz to ./my_data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./my_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f2205c1b5b943c39bafab0b88f67f90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./my_data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./my_data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./my_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "747259f33ef5460c9dc245939d1ec9b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./my_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./my_data/MNIST/raw\n",
      "\n",
      "Min Pixel Value: 0 \n",
      "Max Pixel Value: 255\n",
      "Mean Pixel Value 33.31842041015625 \n",
      "Pixel Values Std: 78.56748962402344\n",
      "Scaled Mean Pixel Value 0.13066047430038452 \n",
      "Scaled Pixel Values Std: 0.30810779333114624\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='./my_data/',\n",
    "                                        train=True, \n",
    "                                        transform=transform,\n",
    "                                        download=True)\n",
    "\n",
    "print('Min Pixel Value: {} \\nMax Pixel Value: {}'.format(train_dataset.data.min(), train_dataset.data.max()))\n",
    "print('Mean Pixel Value {} \\nPixel Values Std: {}'.format(train_dataset.data.float().mean(), train_dataset.data.float().std()))\n",
    "print('Scaled Mean Pixel Value {} \\nScaled Pixel Values Std: {}'.format(train_dataset.data.float().mean() / 255, train_dataset.data.float().std() / 255))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On constate que la moyenne et la variance n'ont pas changé. En effet, la normalisation ne sera appliquée qu'au moment de l'utilisation du dataset.\n",
    "\n",
    "On complète le code par un dataset de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='my_data/',\n",
    "                                            train=True, \n",
    "                                            transform=transform,\n",
    "                                            download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='my_data/',\n",
    "                                            train=False, \n",
    "                                            transform=transform,\n",
    "                                            download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois les datasets définis, on peut définir les DataLoaders correspondants.\n",
    "\n",
    "On peut également définir un certain nombre d'autres informations au niveau du DataLoader, notamment :\n",
    "- \"batch_size\" : taille du batch, c'est à dire :\n",
    "    - pour les données d'apprentissage, le nombre d'images présentées en entrée du réseau avant l'adaptation de ses poids au moyen de la règle d'apprentissage\n",
    "    - pour les données de test, le nombre d'images présentées en entrée du réseau avant le calcul du taux de reconnaissance (\"Accuracy\")\n",
    "- \"shuffle\" : mélange aléatoire des images avant tirage ou pas\n",
    "- etc.  \n",
    "\n",
    "Ici, on choisit de mélanger les données d'apprentissage mais pas les données de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "print(len(train_loader.dataset))\n",
    "print(len(test_loader.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Création du réseau de neurones</b>\n",
    "\n",
    "En première approche, on choisit une structure simple composée d'une seule couche convolutive et d'une couche complètement connectée.  \n",
    "Cette structure est la plus simple qu'on puisse imaginer pour un réseau de neurone convolutif, mais elle ne donnera forcément pas de très bon résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = 8                #nombre de feature-maps pour la couche convolutive\n",
    "nb_cl = 10            #nombre de classes\n",
    "im_w, im_h = 28, 28\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.name = \"CNN MNIST\"\n",
    "        self.conv = nn.Conv2d(in_channels=1, out_channels=ft, kernel_size=3, padding=1)\n",
    "        self.fc = nn.Linear(im_w*im_h*ft, nb_cl)\n",
    "        #self.pool = nn.MaxPool2d(2, 2)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv(x))\n",
    "        #x = self.pool(x)\n",
    "        x = torch.flatten(x, 1)     #aplatissement (2D->1D)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "model = ConvNet().to(device)      #creation du modèle (=reseau de neurones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Affichage de la structure du réseau</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Conv2d: 1-1                            [-1, 8, 28, 28]           80\n",
      "├─Linear: 1-2                            [-1, 10]                  62,730\n",
      "==========================================================================================\n",
      "Total params: 62,810\n",
      "Trainable params: 62,810\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 0.12\n",
      "==========================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.05\n",
      "Params size (MB): 0.24\n",
      "Estimated Total Size (MB): 0.29\n",
      "==========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:51] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "├─Conv2d: 1-1                            [-1, 8, 28, 28]           80\n",
       "├─Linear: 1-2                            [-1, 10]                  62,730\n",
       "==========================================================================================\n",
       "Total params: 62,810\n",
       "Trainable params: 62,810\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.12\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.05\n",
       "Params size (MB): 0.24\n",
       "Estimated Total Size (MB): 0.29\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(model, (1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Fonction de coût (\"loss\") et optimiseur (\"optimizer\")</b>\n",
    "\n",
    "Pour l'apprentissage, on doit choisir la fonction de coût (loss) à utiliser. Il en existe différents types.  \n",
    "Il existe également plusieurs type d'optimiseurs. L'optimiseur est l'algorithme permettant de réduire le loss à chaque itération d'apprentissage.  \n",
    "Parmi les paramètres d'apprentissage, on trouve le taux d'apprentissage (learning rate), qui ajuste la quantité de modification des poids du réseau de neurones à chaque itération d'apprentissage.  \n",
    "Dans cet exemple on choisit pour la fonction de coût l'entropie croisée, et pour l'optimiseur Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#optimizer = torch.optim.Adadelta(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Fonction d'apprentissage</b>\n",
    "\n",
    "On écrit une fonction d'apprentissage pour les boucles sur les batches d'images.\n",
    "Rappel de l'algorithme d'apprentissage :  \n",
    "\n",
    "- présentation d'un batch d'images d'apprentissage en entrée du réseau\n",
    "- calcul des sorties correspondantes du réseau\n",
    "- calcul de l'erreur de sortie\n",
    "- modification des poids du réseau de neurones en fonction de cette erreur, de la dernière couche à la première (*)\n",
    "\n",
    "(*)\"rétro-propagation de l'erreur\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    data_len = len(train_loader.dataset)\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)         # calcul de l'erreur de sortie\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 200 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), data_len, 100. * batch_idx / len(train_loader), loss.item()))\n",
    "        train_loss += loss.item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)          # get the index of the max log-probability\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()  \n",
    "    train_loss /= data_len\n",
    "    accuracy = 100. * correct / len(train_loader.dataset)\n",
    "    print('Train Epoch: {} [{}/{}]\\tLoss: {:.4f}\\tTrain accuracy: {:.4f}'.format(\n",
    "                epoch, batch_idx * len(data), data_len, loss.item(), accuracy))\n",
    "    return train_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Fonction de test</b>\n",
    "\n",
    "La fonction de test réalise l'activation du réseau de neurones (c'est à dire de ses couches, de la première à la dernière), mais cette fois-ci sans appliquer l'apprentissage. Cela permet de comparer sa sortie réelle à la sortie désirée, afin d'évaluer ses performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)         # calcul de l'erreur\n",
    "            test_loss += loss.item()\n",
    "            #test_loss += criterion(output, target)         # calcul de l'erreur\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    #print('\\nTest set: Average loss: {:.4f}, Test accuracy: ({:.0f}%)\\n'.format(loss, accuracy))\n",
    "    return test_loss, accuracy\n",
    "\"\"\"\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)         # calcul de l'erreur\n",
    "            test_loss += loss.item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Test accuracy: ({:.2f}%)\\n'.format(test_loss, accuracy))\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut alors lancer la boucle d'apprentissage sur les époques.  \n",
    "Le fait de faire le test à chaque itération de cette boucle permet d'obtenir le loss et l'accuracy sur les données de test (dans le but de voir leur évolution au fil des itérations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.338049\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.151195\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.062311\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.206111\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.186269\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.272521\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.075881\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.195725\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.065159\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.143725\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.162820\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.034838\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.233494\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.143908\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.207995\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.062447\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.086640\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.017013\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.048098\n",
      "Train Epoch: 1 [59984/60000]\tLoss: 0.1035\tTrain accuracy: 94.5483\n",
      "\n",
      "Test set: Average loss: 0.0052, Test accuracy: (97.45%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.001805\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.228712\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.039922\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.042240\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.005727\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.236507\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.138188\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.018813\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.043412\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.031013\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.005086\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.016737\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.007140\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.007067\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.006278\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.095523\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.074637\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.006994\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.006242\n",
      "Train Epoch: 2 [59984/60000]\tLoss: 0.0843\tTrain accuracy: 97.6850\n",
      "\n",
      "Test set: Average loss: 0.0050, Test accuracy: (97.41%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.088356\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.048317\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.009749\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.061071\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.004569\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.002214\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.056738\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.000433\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.092282\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.010730\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.025545\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.001999\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.005464\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.019070\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.092297\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.001041\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.004120\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.127026\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.015393\n",
      "Train Epoch: 3 [59984/60000]\tLoss: 0.1577\tTrain accuracy: 98.3067\n",
      "\n",
      "Test set: Average loss: 0.0059, Test accuracy: (97.27%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.044049\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.026710\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.000180\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.002839\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.134524\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.001062\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.030203\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.016972\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.001903\n",
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.006006\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.001245\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.162240\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.001238\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.073357\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.200779\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.042427\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.006433\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.128908\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.004834\n",
      "Train Epoch: 4 [59984/60000]\tLoss: 0.0164\tTrain accuracy: 98.7033\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "train_losses, test_losses, train_accuracies, test_accuracies = [], [], [], []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss, train_accuracy = train(model, device, train_loader, optimizer, epoch)\n",
    "    test_loss, test_accuracy = test(model, device, test_loader)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accuracies.append(test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Affichage graphique des résultats</b>\n",
    "\n",
    "Il est toujours intéressant d'afficher graphiquement la progression de l'apprentissage, notamment la précision sur les données d'apprentissage et sur les données de test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(train_losses)\n",
    "print(test_losses)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "\n",
    "plt.title(\"loss vs learning epochs on MNIST dataset\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.plot(train_losses, label='Train')\n",
    "plt.plot(test_losses, label='Test')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.subplot(2, 1, 2)\n",
    "\n",
    "plt.title(\"accuracy vs learning epochs on MNIST dataset\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.plot(train_accuracies, label='Train')\n",
    "plt.plot(test_accuracies, label='Test')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On constate que la précision sur les données de test est très bonne dès les premières époques d'apprentissage, et ce malgré la structure très simple du réseau et son faible nombre de paramètres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Sauvegarde du modèle</b>\n",
    "\n",
    "On peut sauvegarder le modèle, pour utilisation ultérieure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Utilisation ultérieure : chargement du modèle généré précédemment et inférence</b>\n",
    "\n",
    "Par inférence, on entend la présentation d'une image en entrée du modèle et le calcul de la prédiction de ce dernier.  \n",
    "L'utilisation du réseau de neurones en inférence consiste à charger le fichier-modèle, puis à appliquer le modèle à des images de test.  \n",
    "\n",
    "Remarque : pour être sûr que le modèle présent en mémoire soit celui qui est chargé du fichier, il faut redémarrer le noyau du Notebook.  \n",
    "Dans ce cas, il faut relancer une partie du code défini plus haut. Plus précisément :\n",
    "\n",
    "- Importation des librairies nécessaires\n",
    "- Définition de la structure du réseau de neurones ainsi que son flot d'information\n",
    "- Création d'un réseau de neurones et adaptation au processeur utilisé\n",
    "\n",
    "Pour le test, on peut récupérer une image PNG directement du github suivant :\n",
    "\n",
    "https://github.com/Paperspace/mnist-sample/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp example5.png test.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def predict(model, image):\n",
    "    image = image.resize((im_w, im_h))\n",
    "    np_img = np.array(image)                        # conversion en tableau numpy\n",
    "    image_tensor = torch.from_numpy(np_img)         # conversion en tenseur PyTorch\n",
    "    print(image_tensor.shape)\n",
    "    image_tensor = image_tensor.reshape([1, 1, np_img.shape[0], np_img.shape[1]]) # ajout d'une dimension (cf + haut)\n",
    "    output = model(image_tensor.float())        # application de l'image en entrée du réseau\n",
    "    index = output.data.numpy().argmax()\n",
    "    return index\n",
    "\n",
    "#programme principal\n",
    "path = './'\n",
    "image_file = 'test.png'\n",
    "true_index = 5             #classe \"30\" : index = 0 ; classe \"50\" : index = 1\n",
    "\n",
    "device = 'cpu'\n",
    "print(\"device : \", device)\n",
    "\n",
    "model = ConvNet().to(device)    #creation d'un nouvelle instance du modele\n",
    "model.load_state_dict(torch.load('model.ckpt'))    # chargement des poids sauvegardés précédemment\n",
    "model.eval()\n",
    "\n",
    "img = Image.open(path + image_file)  # image au hasard\n",
    "img = img.convert('L')               #conversion en monochrome\n",
    "index = predict(model, img)\n",
    "print('index estimé : ', index)\n",
    "\n",
    "#affichage du résultat sur l'image\n",
    "classe = [i for i in range(10)]    #nom des classes\n",
    "\n",
    "if(true_index == index):\n",
    "    res = True\n",
    "else:\n",
    "    res = False\n",
    "plt.title('classe : ' + str(classe[index]) + ' (' + str(res) + ')')\n",
    "plt.axis('off')\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Données GTSRB</b>\n",
    "\n",
    "Les données GTSRB doivent être chargées depuis un emplacement local. C'est donc un bon exemple, similaire à ce qu'on serait amené à faire dans un cas d'utilisation réelle, avec ses propres images.  \n",
    "\n",
    "Le système de DataLoader de Pytorch permet de charger les données depuis un emplacement local. Il existe également une fonction \"random_split()\" permettant de séparer un ensemble de données en deux sous-ensembles (apprentissage/test).  \n",
    "La solution la plus simple consiste donc à ne charger que les données d'apprentissage, puis de les séparer en deux.\n",
    "\n",
    "#### <b>Chargement des librairies utiles et détection de présence d'un GPU</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSs\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "#détection de présence d'un CPU\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'data/GTSRB/Final_Training/Images/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Chargement des données</b>\n",
    "\n",
    "Contrairement à MNIST, on n'a pas ici un dataset d'apprentissage et un dataset de test.\n",
    "On utilise donc la fonction de séparation de Pytorch (\"random_split\") pour séparer le dataset disponible en 2 parties.\n",
    "\n",
    "On définit donc :\n",
    " - les transformations à appliquer aux données\n",
    " - la séparation train/test (ou validation)\n",
    " - les 2 DataLoaders correspondants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39209\n",
      "31367\n",
      "7842\n"
     ]
    }
   ],
   "source": [
    "im_w, im_h = 28, 28\n",
    "batch_size = 64\n",
    "\n",
    "transform = transforms.Compose([\n",
    "                               transforms.Resize((im_w, im_h)),\n",
    "                               transforms.ToTensor(),   #car la transformation qui suit porte sur des tenseurs\n",
    "                               transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])\n",
    "                               ])\n",
    "\n",
    "full_dataset = datasets.ImageFolder(train_dir, transform=transform)\n",
    "\n",
    "#split dataset for train/test\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "print(len(full_dataset))\n",
    "print(len(train_dataset))\n",
    "print(len(val_dataset))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Création du réseau de neurones</b>\n",
    "\n",
    "Dans un premier temps, on reprend la structure générale du réseau de neurones déjà utilisé avec MNIST, mais il faut adapter le nombre de classes.\n",
    "\n",
    "On utilise une classe pour définir la structure du réseau de neurones et son flot d'information, puis on crée un réseau de neurones proprement dit (= une instance de la classe), que l'on adapte au processeur utilisé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ft = 8        #nombre de feature-maps de la couche convolutive\n",
    "nb_classes = 43\n",
    "\n",
    "#avec une couche de convolution, une couche complètement connectée\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.name = \"CNN for GTSRB\"\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=ft, kernel_size=3, padding=2) #30*30 to 28*28\n",
    "        #self.fc = nn.Linear(im_w*im_h*ft, nb_classes)     \n",
    "        #self.conv2 =nn.Conv2d(1, ft, kernel_size=5, padding=2)\n",
    "        self.pool1 =nn.MaxPool2d(kernel_size=2, stride=2) #14*14\n",
    "        self.conv3 = nn.Conv2d(ft, 64, kernel_size=3) #12*12\n",
    "        self.pool2= nn.MaxPool2d(kernel_size=2, stride=2)   #6*6 \n",
    "        self.fc = nn.Linear(64*6*6, 120)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.fc1 =nn.Linear(120, 84)\n",
    "        self.fc2 =nn.Linear(84, nb_classes)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))  \n",
    "        #x = F.relu(self.conv2(x))\n",
    "        x=self.pool1(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x=self.pool2(x)\n",
    "        x = torch.flatten(x, 1)     #aplatissement\n",
    "        x = self.fc(x)\n",
    "        x =self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        nn.Flatten()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Création d'un réseau de neurones et adaptation au processeur utilisé</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(8, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc): Linear(in_features=2304, out_features=120, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (fc1): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc2): Linear(in_features=84, out_features=43, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Net().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Affichage de la structure du réseau</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Conv2d: 1-1                            [-1, 8, 30, 30]           224\n",
      "├─MaxPool2d: 1-2                         [-1, 8, 15, 15]           --\n",
      "├─Conv2d: 1-3                            [-1, 64, 13, 13]          4,672\n",
      "├─MaxPool2d: 1-4                         [-1, 64, 6, 6]            --\n",
      "├─Linear: 1-5                            [-1, 120]                 276,600\n",
      "├─Dropout: 1-6                           [-1, 120]                 --\n",
      "├─Linear: 1-7                            [-1, 84]                  10,164\n",
      "├─Linear: 1-8                            [-1, 43]                  3,655\n",
      "==========================================================================================\n",
      "Total params: 295,315\n",
      "Trainable params: 295,315\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 1.26\n",
      "==========================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.14\n",
      "Params size (MB): 1.13\n",
      "Estimated Total Size (MB): 1.27\n",
      "==========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:51] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "├─Conv2d: 1-1                            [-1, 8, 30, 30]           224\n",
       "├─MaxPool2d: 1-2                         [-1, 8, 15, 15]           --\n",
       "├─Conv2d: 1-3                            [-1, 64, 13, 13]          4,672\n",
       "├─MaxPool2d: 1-4                         [-1, 64, 6, 6]            --\n",
       "├─Linear: 1-5                            [-1, 120]                 276,600\n",
       "├─Dropout: 1-6                           [-1, 120]                 --\n",
       "├─Linear: 1-7                            [-1, 84]                  10,164\n",
       "├─Linear: 1-8                            [-1, 43]                  3,655\n",
       "==========================================================================================\n",
       "Total params: 295,315\n",
       "Trainable params: 295,315\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 1.26\n",
       "==========================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 0.14\n",
       "Params size (MB): 1.13\n",
       "Estimated Total Size (MB): 1.27\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(model, (3, im_w, im_h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Fonction d'apprentissage</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    data_len = len(train_loader.dataset)\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()                              #initialisation des gradients\n",
    "        output = model(data)                               #activation du réseau\n",
    "        loss = criterion(output, target)                   #calcul de l'erreur de sortie\n",
    "        loss.backward()                                    #rétro-propagation du gradient\n",
    "        optimizer.step()                                   #adaptation des poids des neurones\n",
    "        train_loss += loss.item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)          # get the index of the max log-probability\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        if batch_idx % 200 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.4f}'.format(\n",
    "                epoch, batch_idx * len(data), data_len, 100.*batch_idx/len(train_loader), loss.item()))\n",
    "    train_loss /= data_len\n",
    "    accuracy = 100. * correct / data_len\n",
    "    print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tTrain accuracy: {:.4f}'.format(\n",
    "                epoch, batch_idx*len(data), data_len, 100.*batch_idx/data_len, accuracy))\n",
    "    return train_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Fonction de test</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)         # calcul de l'erreur\n",
    "            test_loss += loss.item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Test accuracy: ({:.2f}%)\\n'.format(test_loss, accuracy))\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Conv2d: 1-1                            [-1, 8, 30, 30]           224\n",
      "├─MaxPool2d: 1-2                         [-1, 8, 15, 15]           --\n",
      "├─Conv2d: 1-3                            [-1, 64, 13, 13]          4,672\n",
      "├─MaxPool2d: 1-4                         [-1, 64, 6, 6]            --\n",
      "├─Linear: 1-5                            [-1, 120]                 276,600\n",
      "├─Dropout: 1-6                           [-1, 120]                 --\n",
      "├─Linear: 1-7                            [-1, 84]                  10,164\n",
      "├─Linear: 1-8                            [-1, 43]                  3,655\n",
      "==========================================================================================\n",
      "Total params: 295,315\n",
      "Trainable params: 295,315\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 1.26\n",
      "==========================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.14\n",
      "Params size (MB): 1.13\n",
      "Estimated Total Size (MB): 1.27\n",
      "==========================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "├─Conv2d: 1-1                            [-1, 8, 30, 30]           224\n",
       "├─MaxPool2d: 1-2                         [-1, 8, 15, 15]           --\n",
       "├─Conv2d: 1-3                            [-1, 64, 13, 13]          4,672\n",
       "├─MaxPool2d: 1-4                         [-1, 64, 6, 6]            --\n",
       "├─Linear: 1-5                            [-1, 120]                 276,600\n",
       "├─Dropout: 1-6                           [-1, 120]                 --\n",
       "├─Linear: 1-7                            [-1, 84]                  10,164\n",
       "├─Linear: 1-8                            [-1, 43]                  3,655\n",
       "==========================================================================================\n",
       "Total params: 295,315\n",
       "Trainable params: 295,315\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 1.26\n",
       "==========================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 0.14\n",
       "Params size (MB): 1.13\n",
       "Estimated Total Size (MB): 1.27\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(model, (3, im_w, im_h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Paramétrage de l'apprentissage et du test</b>\n",
    "\n",
    "Pour l'apprentissage, on doit choisir la fonction de coût (loss) utilisée. Il en existe différents types.  \n",
    "Il existe également plusieurs type d'optimiseurs. L'optimiseur est l'algorithme permettant de réduire le loss à chaque itération d'apprentissage.  \n",
    "Parmi les paramètres d'apprentissage, on trouve également le taux d'apprentissage (learning rate), qui ajuste la quantité de modification des poids du réseau de neurones à chaque itération d'apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)    #utilisé par l'apprentissage (lr <-> learning rate)\n",
    "criterion = nn.CrossEntropyLoss()       #critère d'erreur en sortie (utilisé par l'apprentissage et le test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Boucle d'apprentissage</b>\n",
    "\n",
    "Attention, l'apprentissage avec le dataset complet prend un peu de temps (environ 1 mn par épisode sur un CPU récent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning (please wait)...\n",
      "Train Epoch: 1 [0/31367 (0%)]\tLoss: 3.7400\n",
      "Train Epoch: 1 [12800/31367 (41%)]\tLoss: 0.9195\n",
      "Train Epoch: 1 [25600/31367 (81%)]\tLoss: 0.3254\n",
      "Train Epoch: 1 [3430/31367 (2%)]\tTrain accuracy: 69.4424\n",
      "\n",
      "Test set: Average loss: 0.0050, Test accuracy: (90.77%)\n",
      "\n",
      "Train Epoch: 2 [0/31367 (0%)]\tLoss: 0.2223\n",
      "Train Epoch: 2 [12800/31367 (41%)]\tLoss: 0.2287\n",
      "Train Epoch: 2 [25600/31367 (81%)]\tLoss: 0.0819\n",
      "Train Epoch: 2 [3430/31367 (2%)]\tTrain accuracy: 94.2296\n",
      "\n",
      "Test set: Average loss: 0.0020, Test accuracy: (97.16%)\n",
      "\n",
      "Train Epoch: 3 [0/31367 (0%)]\tLoss: 0.1134\n",
      "Train Epoch: 3 [12800/31367 (41%)]\tLoss: 0.0461\n",
      "Train Epoch: 3 [25600/31367 (81%)]\tLoss: 0.0964\n",
      "Train Epoch: 3 [3430/31367 (2%)]\tTrain accuracy: 97.0989\n",
      "\n",
      "Test set: Average loss: 0.0018, Test accuracy: (97.22%)\n",
      "\n",
      "Train Epoch: 4 [0/31367 (0%)]\tLoss: 0.0864\n",
      "Train Epoch: 4 [12800/31367 (41%)]\tLoss: 0.0661\n",
      "Train Epoch: 4 [25600/31367 (81%)]\tLoss: 0.0790\n",
      "Train Epoch: 4 [3430/31367 (2%)]\tTrain accuracy: 97.8608\n",
      "\n",
      "Test set: Average loss: 0.0018, Test accuracy: (97.13%)\n",
      "\n",
      "Train Epoch: 5 [0/31367 (0%)]\tLoss: 0.0271\n",
      "Train Epoch: 5 [12800/31367 (41%)]\tLoss: 0.0321\n",
      "Train Epoch: 5 [25600/31367 (81%)]\tLoss: 0.0205\n",
      "Train Epoch: 5 [3430/31367 (2%)]\tTrain accuracy: 98.5048\n",
      "\n",
      "Test set: Average loss: 0.0010, Test accuracy: (98.67%)\n",
      "\n",
      "Train Epoch: 6 [0/31367 (0%)]\tLoss: 0.0757\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "test_losses, test_accuracies, train_losses, train_accuracies = [], [], [], []\n",
    "print('Learning (please wait)...')\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss, train_accuracy = train(model, device, train_loader, optimizer, epoch)\n",
    "    test_loss, test_accuracy = test(model, device, test_loader)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Affichage du loss et du taux de reconnaissance sur les données d'apprentissage et de test</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "\n",
    "plt.title(\"loss vs learning epochs on GTSRB dataset\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.plot(train_losses, label='Train')\n",
    "plt.plot(test_losses, label='Test')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.subplot(2, 1, 2)\n",
    "\n",
    "plt.title(\"accuracy vs learning epochs on GTSRB dataset\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.plot(train_accuracies, label='Train')\n",
    "plt.plot(test_accuracies, label='Test')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Là encore, les résultats sont bons malgré la simplicité du modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Sauvegarde du modèle</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print('model saved')\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce format de sauvegarde comporte les poids du réseau de neurones, mais également les paramètres d'apprentissage, les valeurs du loss, etc.\n",
    "Ce fichier peut donc servir de base à la reprise de l'apprentissage. On parle alors de checkpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Utilisation ultérieure : chargement du modèle généré précédemment et inférence (Exercice 4.2)</b>\n",
    "\n",
    "Par inférence, on entend la présentation d'une image en entrée du modèle et le calcul de la prédiction de ce dernier.  \n",
    "L'utilisation du réseau de neurones en inférence consiste à charger le fichier-modèle, puis l'application à des images de test.  \n",
    "\n",
    "Remarque : pour être sûr que le modèle présent en mémoire soit celui qui est chargé du fichier, il faut redémarrer le noyau du Notebook.  \n",
    "Dans ce cas, il faut relancer relancer une partie du code défini plus haut. Plus précisément :\n",
    "\n",
    "- Importation des librairies nécessaires\n",
    "- Définition de la structure du réseau de neurones ainsi que son flot d'information\n",
    "- Création d'un réseau de neurones et adaptation au processeur utilisé\n",
    "\n",
    "Pour simplifier le programme d'inférence, on peut utiliser une fonction.\n",
    "Ainsi dans une boucle d'acquisition d'images par exemple, l'inférence se résume à un appel de cette fonction.\n",
    "\n",
    "##### <b>Test sur quelques images</b>\n",
    "\n",
    "L'exemple ci-dessous charge quelques images du dataset de test, les adapte au format tenseur de torch puis les applique en entrée du modèle.  \n",
    "Puis on affiche sur l'image si la réponse est correcte ou non"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def predict(model, image):\n",
    "    image = image.resize((im_w, im_h))\n",
    "    np_img = np.array(image)                        # conversion en tableau numpy\n",
    "    image_tensor = torch.from_numpy(np_img)         # conversion en tenseur PyTorch\n",
    "    print(image_tensor.shape)\n",
    "    image_tensor = image_tensor.reshape([1, 1, np_img.shape[0], np_img.shape[1]]) # ajout d'une dimension (cf + haut)\n",
    "    output = model(image_tensor.float())        # application de l'image en entrée du réseau\n",
    "    index = output.data.numpy().argmax()\n",
    "    return index\n",
    "\n",
    "#programme principal\n",
    "path = './'\n",
    "image_file = 'test.png'\n",
    "true_index = 5             #classe \"30\" : index = 0 ; classe \"50\" : index = 1\n",
    "\n",
    "device = 'cpu'\n",
    "print(\"device : \", device)\n",
    "\n",
    "model = ConvNet().to(device)    #creation d'un nouvelle instance du modele\n",
    "model.load_state_dict(torch.load('model.ckpt'))    # chargement des poids sauvegardés précédemment\n",
    "model.eval()\n",
    "\n",
    "img = Image.open(path + image_file)  # image au hasard\n",
    "img = img.convert('L')               #conversion en monochrome\n",
    "index = predict(model, img)\n",
    "print('index estimé : ', index)\n",
    "\n",
    "#affichage du résultat sur l'image\n",
    "classe = [i for i in range(10)]    #nom des classes\n",
    "\n",
    "if(true_index == index):\n",
    "    res = True\n",
    "else:\n",
    "    res = False\n",
    "plt.title('classe : ' + str(classe[index]) + ' (' + str(res) + ')')\n",
    "plt.axis('off')\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Test sur tout le dataset de test</b>\n",
    "\n",
    "On peut également vouloir faire un test sur toute la base de test.\n",
    "Dans ce cas il faut exécuter en plus :\n",
    "\n",
    "- le chargement des données\n",
    "- la fonction de test\n",
    "- le paramétrage du test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La précision est plutôt bonne sur l'ensemble de test, mais on peut faire mieux en jouant sur différents paramètres du modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Exercice 4.3</b>\n",
    "\n",
    "On peut reprendre l'exemple complet de classification des données GTSRB avec un MLP.\n",
    "Adapter le programme suivant pour obtenir une <b>précision (accuracy) >=99,5%</b> avec un réseau de neurones convolutif comportant <b>moins de 500000 paramètres</b>.\n",
    "On pourra jouer sur :\n",
    "- le nombre de paramètres\n",
    "- la taille des images\n",
    "- la taille des batchs\n",
    "- l'augmentation de la base d'apprentissage\n",
    "- le dropout\n",
    "- la batch-normalisation\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Références</b>\n",
    "\n",
    "- Image processing with numpy :\n",
    "https://pythoninformer.com/python-libraries/numpy/numpy-and-images/\n",
    "- Convolutional Neural Networks : \n",
    "https://www.cs.toronto.edu/~lczhang/360/lec/w04/convnet.html \n",
    "- PyTorch tutorials : \n",
    "https://github.com/pytorch/examples/tree/master/mnist\n",
    "- Basic PyTorch operations :\n",
    "https://subscription.packtpub.com/book/big_data_and_business_intelligence/9781789534092/1/ch01lvl1sec11/installing-pytorch\n",
    "- MNIST Handwritten Digit Recognition in PyTorch :\n",
    "https://nextjournal.com/gkoehler/pytorch-mnist\n",
    "- PyTorch for Beginners: Image Classification using Pre-trained models : \n",
    "https://www.learnopencv.com/pytorch-for-beginners-image-classification-using-pre-trained-models/\n",
    "- PyTorch pretrained models (AlexNet example) : \n",
    "https://pytorch.org/hub/pytorch_vision_alexnet/\n",
    "- Transfert learning for image classification\n",
    "https://curiousily.com/posts/transfer-learning-for-image-classification-using-torchvision-pytorch-and-python/\n",
    "- Pytorch classification tutorial (CIFAR images) :\n",
    "https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "- Face Recognition project in PyTorch using CNNs :\n",
    "https://github.com/apsdehal/Face-Recognition\n",
    "- Image classification with PyTorch\n",
    "https://www.pluralsight.com/guides/image-classification-with-pytorch\n",
    "- How to Train an Image Classifier in PyTorch and use it to Perform Basic Inference on Single Image :\n",
    "https://towardsdatascience.com/how-to-train-an-image-classifier-in-pytorch-and-use-it-to-perform-basic-inference-on-single-images-99465a1e9bf5\n",
    "- Saving and Loading Models in PyTorch :\n",
    "https://github.com/pytorch/tutorials/blob/master/beginner_source/saving_loading_models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Format de la Cellule Texte Brut",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
